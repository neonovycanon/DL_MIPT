{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c379396e-208d-42e0-ba18-513b57160d7f",
   "metadata": {},
   "source": [
    "# Coding transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9510c88a-086c-4b0c-be06-3591f3dbd5ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:01.237634Z",
     "iopub.status.busy": "2025-03-29T10:02:01.237634Z",
     "iopub.status.idle": "2025-03-29T10:02:07.104956Z",
     "shell.execute_reply": "2025-03-29T10:02:07.104956Z",
     "shell.execute_reply.started": "2025-03-29T10:02:01.237634Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54883ed-d855-4cc3-a6a8-d17c0d50021a",
   "metadata": {},
   "source": [
    "## Data retrieval\n",
    "Retrieve training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41381d04-acb6-435e-83d0-312bee8baf9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.104956Z",
     "iopub.status.busy": "2025-03-29T10:02:07.104956Z",
     "iopub.status.idle": "2025-03-29T10:02:07.110881Z",
     "shell.execute_reply": "2025-03-29T10:02:07.110322Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.104956Z"
    }
   },
   "outputs": [],
   "source": [
    "# download the tiny shakespeare dataset\n",
    "\n",
    "filename = 'input.txt'\n",
    "file_dir = os.path.join(os.getcwd(), 'data')\n",
    "file_path = os.path.join(file_dir, filename)\n",
    "\n",
    "if not os.path.exists(file_dir):\n",
    "    os.mkdir(file_dir)\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(data_url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87897303-5452-49a1-96e0-5bf5516ee562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.112054Z",
     "iopub.status.busy": "2025-03-29T10:02:07.110881Z",
     "iopub.status.idle": "2025-03-29T10:02:07.145414Z",
     "shell.execute_reply": "2025-03-29T10:02:07.145414Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.112054Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86d1665-1ea7-4327-962c-b3c8650fd3a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.145414Z",
     "iopub.status.busy": "2025-03-29T10:02:07.145414Z",
     "iopub.status.idle": "2025-03-29T10:02:07.153445Z",
     "shell.execute_reply": "2025-03-29T10:02:07.152437Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.145414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print('Dataset length', len(text), end = '\\n')\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d22678-2451-4bef-af2c-939d29c332a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.154976Z",
     "iopub.status.busy": "2025-03-29T10:02:07.154976Z",
     "iopub.status.idle": "2025-03-29T10:02:07.167268Z",
     "shell.execute_reply": "2025-03-29T10:02:07.167268Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.154976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Create a simple token space using python inner methods\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "batch_size = 32\n",
    "n_embed = 32\n",
    "print('Dictionary size', vocab_size)\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b3ca5-c2a2-4ee2-9fb2-fdd142e73820",
   "metadata": {},
   "source": [
    "## Tokenizer strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46918859-9615-48be-9684-7f561dbfb10e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.168274Z",
     "iopub.status.busy": "2025-03-29T10:02:07.168274Z",
     "iopub.status.idle": "2025-03-29T10:02:07.171081Z",
     "shell.execute_reply": "2025-03-29T10:02:07.171081Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.168274Z"
    }
   },
   "outputs": [],
   "source": [
    "text2ind = {j: i for i, j in enumerate(chars)}\n",
    "ind2text = {i: j for i, j in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "050ab9dd-4758-4029-b09c-3f335552b744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.172088Z",
     "iopub.status.busy": "2025-03-29T10:02:07.172088Z",
     "iopub.status.idle": "2025-03-29T10:02:07.176431Z",
     "shell.execute_reply": "2025-03-29T10:02:07.176105Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.172088Z"
    }
   },
   "outputs": [],
   "source": [
    "encode = lambda x: [text2ind[char] for char in x]\n",
    "decode = lambda x: [ind2text[ind] for ind in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "293a9485-7c29-49bd-8579-0cfc0a60e153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.177396Z",
     "iopub.status.busy": "2025-03-29T10:02:07.177396Z",
     "iopub.status.idle": "2025-03-29T10:02:07.180505Z",
     "shell.execute_reply": "2025-03-29T10:02:07.180505Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.177396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 47, 1, 58, 46, 43, 56, 43]\n",
      "['H', 'i', ' ', 't', 'h', 'e', 'r', 'e']\n"
     ]
    }
   ],
   "source": [
    "print(encode('Hi there'))\n",
    "print(decode(encode('Hi there')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5072a67-3d81-44d7-9aa7-8d433c816d38",
   "metadata": {},
   "source": [
    "Tokenizer uses simple encoding and decoding strategies that represent simple look-up tables that operate only at char level (because we're working with simple char-level transformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a720a87-fa15-40df-b195-eb0c656c3610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.181577Z",
     "iopub.status.busy": "2025-03-29T10:02:07.181577Z",
     "iopub.status.idle": "2025-03-29T10:02:07.265275Z",
     "shell.execute_reply": "2025-03-29T10:02:07.265275Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.181577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long) # torch.long represents int64 \n",
    "print(data.shape, data.dtype) # -> Shape of a known dataset that was seen previously\n",
    "\n",
    "print(data[:50]) # -> Encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee3c1a28-9cd2-4960-ba83-97323a9d78cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.266278Z",
     "iopub.status.busy": "2025-03-29T10:02:07.266278Z",
     "iopub.status.idle": "2025-03-29T10:02:07.270699Z",
     "shell.execute_reply": "2025-03-29T10:02:07.269901Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.266278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "257213e5-218b-44ef-b4e1-c96b8957deca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.270699Z",
     "iopub.status.busy": "2025-03-29T10:02:07.270699Z",
     "iopub.status.idle": "2025-03-29T10:02:07.279434Z",
     "shell.execute_reply": "2025-03-29T10:02:07.279434Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.270699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # Context length\n",
    "print(train_data[:block_size+1])\n",
    "decode(train_data[:block_size+1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a90b68ee-03ec-408a-8239-da04dcbc1cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.279434Z",
     "iopub.status.busy": "2025-03-29T10:02:07.279434Z",
     "iopub.status.idle": "2025-03-29T10:02:07.282950Z",
     "shell.execute_reply": "2025-03-29T10:02:07.282950Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.279434Z"
    }
   },
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de36e5-a7ab-4b59-8605-eb214a09ca1a",
   "metadata": {},
   "source": [
    "Training data consists of `x` and `y` lists, that are packed each with 8 symbols. For each item in `x` list item in `y` list is considered the following one in context of all preceding `x` items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4541d899-7fbf-4532-85e4-822b219ce511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.283954Z",
     "iopub.status.busy": "2025-03-29T10:02:07.283954Z",
     "iopub.status.idle": "2025-03-29T10:02:07.289993Z",
     "shell.execute_reply": "2025-03-29T10:02:07.289577Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.283954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([18]) corresponds to ouptut 47\n",
      "Input: ['F'] corresponds to ouptut ['i']\n",
      "Input: tensor([18, 47]) corresponds to ouptut 56\n",
      "Input: ['F', 'i'] corresponds to ouptut ['r']\n",
      "Input: tensor([18, 47, 56]) corresponds to ouptut 57\n",
      "Input: ['F', 'i', 'r'] corresponds to ouptut ['s']\n",
      "Input: tensor([18, 47, 56, 57]) corresponds to ouptut 58\n",
      "Input: ['F', 'i', 'r', 's'] corresponds to ouptut ['t']\n",
      "Input: tensor([18, 47, 56, 57, 58]) corresponds to ouptut 1\n",
      "Input: ['F', 'i', 'r', 's', 't'] corresponds to ouptut [' ']\n",
      "Input: tensor([18, 47, 56, 57, 58,  1]) corresponds to ouptut 15\n",
      "Input: ['F', 'i', 'r', 's', 't', ' '] corresponds to ouptut ['C']\n",
      "Input: tensor([18, 47, 56, 57, 58,  1, 15]) corresponds to ouptut 47\n",
      "Input: ['F', 'i', 'r', 's', 't', ' ', 'C'] corresponds to ouptut ['i']\n",
      "Input: tensor([18, 47, 56, 57, 58,  1, 15, 47]) corresponds to ouptut 58\n",
      "Input: ['F', 'i', 'r', 's', 't', ' ', 'C', 'i'] corresponds to ouptut ['t']\n"
     ]
    }
   ],
   "source": [
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'Input: {context} corresponds to ouptut {target}')\n",
    "    print(f'Input: {decode(context.tolist())} corresponds to ouptut {decode([target.tolist()])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94ef67-cb29-4805-8ad8-62a5826f4072",
   "metadata": {},
   "source": [
    "We enable the transformer to see context for a sentence with a length of 1 to length of context size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "850a0955-2ffd-4a38-a7db-7a1ed5137ba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.291997Z",
     "iopub.status.busy": "2025-03-29T10:02:07.291997Z",
     "iopub.status.idle": "2025-03-29T10:02:07.296535Z",
     "shell.execute_reply": "2025-03-29T10:02:07.295905Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.291997Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1d4b56ee-30b3-4c69-b541-7b05646936db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T11:21:43.772846Z",
     "iopub.status.busy": "2025-03-29T11:21:43.772846Z",
     "iopub.status.idle": "2025-03-29T11:21:43.779454Z",
     "shell.execute_reply": "2025-03-29T11:21:43.779454Z",
     "shell.execute_reply.started": "2025-03-29T11:21:43.772846Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(split, block_size: int = 8):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # -> sample indexes for random sequences\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device) # -> sample sequences with a fixed context length\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device) # -> sample y outputs\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9cac9848-7908-4be5-880a-1ad45a517b2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T11:21:44.356128Z",
     "iopub.status.busy": "2025-03-29T11:21:44.356128Z",
     "iopub.status.idle": "2025-03-29T11:21:44.366962Z",
     "shell.execute_reply": "2025-03-29T11:21:44.365957Z",
     "shell.execute_reply.started": "2025-03-29T11:21:44.356128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdaa6897-0828-42b1-89c1-a87167906189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.380272Z",
     "iopub.status.busy": "2025-03-29T10:02:07.380272Z",
     "iopub.status.idle": "2025-03-29T10:02:07.386116Z",
     "shell.execute_reply": "2025-03-29T10:02:07.385419Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.380272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 't', ' ', 't', 'h', 'a', 't', ' ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(xb[2].to('cpu').tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314694bf-6d05-404b-9e6d-6be7f79697cd",
   "metadata": {},
   "source": [
    "## Sample self-attention blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a8f0d66-b8a2-4be6-b4e6-1b0753d8e212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.386116Z",
     "iopub.status.busy": "2025-03-29T10:02:07.386116Z",
     "iopub.status.idle": "2025-03-29T10:02:07.408725Z",
     "shell.execute_reply": "2025-03-29T10:02:07.408725Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.386116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8,2 # batch, timestemps, channels\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19abd097-925c-40d2-9b91-1694a0ac2062",
   "metadata": {},
   "source": [
    "Creating mechanism that will allow tokens to communicate with each other, by creating self-attention mechanism. This mechanism allows models to attend to earlier context, and omit further context of the message by using triangular matrices.\n",
    "\n",
    "This version of self-attention helps to communicate with earlier tokens by creating average or summed vectors of earlier context. Such process is quite lossy, but it's good enough for simple versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22e8e94e-6ada-4d92-980f-5242057af430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.410680Z",
     "iopub.status.busy": "2025-03-29T10:02:07.408725Z",
     "iopub.status.idle": "2025-03-29T10:02:07.415193Z",
     "shell.execute_reply": "2025-03-29T10:02:07.414685Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.410680Z"
    }
   },
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] #t, C\n",
    "        xbow[b, t] = torch.mean(xprev, 0) #averaging by time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed65ed0-2b05-4f49-af54-c309bdc679a1",
   "metadata": {},
   "source": [
    "This attention type is some averaging process, but very inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5c4988a-f033-48f7-bccc-88fdd4669f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.416199Z",
     "iopub.status.busy": "2025-03-29T10:02:07.415193Z",
     "iopub.status.idle": "2025-03-29T10:02:07.421490Z",
     "shell.execute_reply": "2025-03-29T10:02:07.421490Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.416199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed8a2b2d-ae92-4034-8e85-89150d7b51d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.421490Z",
     "iopub.status.busy": "2025-03-29T10:02:07.421490Z",
     "iopub.status.idle": "2025-03-29T10:02:07.428489Z",
     "shell.execute_reply": "2025-03-29T10:02:07.427105Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.421490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e074e3ea-887d-4bfe-b0a2-76827fd1f33d",
   "metadata": {},
   "source": [
    "Matrix multiplication is the answer for creating a more faster version of an algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bd96c13-1546-4f4a-bfe1-40327281b431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.429017Z",
     "iopub.status.busy": "2025-03-29T10:02:07.429017Z",
     "iopub.status.idle": "2025-03-29T10:02:07.439290Z",
     "shell.execute_reply": "2025-03-29T10:02:07.439290Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.429017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "---\n",
      "b:\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c:\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print('a:', a, sep = '\\n')\n",
    "print('---')\n",
    "print('b:', b, sep = '\\n')\n",
    "print('---')\n",
    "print('c:', c, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "966fe6f0-efc6-4a00-9dd0-4677f8ae116f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.439866Z",
     "iopub.status.busy": "2025-03-29T10:02:07.439866Z",
     "iopub.status.idle": "2025-03-29T10:02:07.450835Z",
     "shell.execute_reply": "2025-03-29T10:02:07.450321Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.439866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721dd29e-4c66-4899-b868-ac399cc2e548",
   "metadata": {},
   "source": [
    "By creating triangular matrix using `tril` we can omit further tokens and work with only preceding ones. Using masked filling we can also work with functions like `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4b041b8-a034-4311-8af5-350eb9f0a066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.450835Z",
     "iopub.status.busy": "2025-03-29T10:02:07.450835Z",
     "iopub.status.idle": "2025-03-29T10:02:07.457945Z",
     "shell.execute_reply": "2025-03-29T10:02:07.457945Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.450835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "---\n",
      "b:\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c:\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) \n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print('a:', a, sep = '\\n')\n",
    "print('---')\n",
    "print('b:', b, sep = '\\n')\n",
    "print('---')\n",
    "print('c:', c, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525ffe2-2f77-4b26-a163-c53e063a6cb6",
   "metadata": {},
   "source": [
    "In order to implement other functions like avearging or softmaxxing we can work with mutated triangular ones matrices.\n",
    "\n",
    "Averaging is done by the following process: lower traingular matrix is mutated to a weight matrix by normalizing it's ones to some weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aedb1b4-2db0-4895-967d-a6d7a1ebf3e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.458839Z",
     "iopub.status.busy": "2025-03-29T10:02:07.458839Z",
     "iopub.status.idle": "2025-03-29T10:02:07.465431Z",
     "shell.execute_reply": "2025-03-29T10:02:07.465431Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.458839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "---\n",
      "b:\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c:\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)) \n",
    "a = a / torch.sum(a, 1, keepdim = True) # Normalize each row to create a weight matrix for further multiplications\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print('a:', a, sep = '\\n')\n",
    "print('---')\n",
    "print('b:', b, sep = '\\n')\n",
    "print('---')\n",
    "print('c:', c, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed1fabf7-f31d-42c8-9ce4-8540a4213920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.466438Z",
     "iopub.status.busy": "2025-03-29T10:02:07.466438Z",
     "iopub.status.idle": "2025-03-29T10:02:07.470528Z",
     "shell.execute_reply": "2025-03-29T10:02:07.470528Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.466438Z"
    }
   },
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] #t, C\n",
    "        xbow[b, t] = torch.mean(xprev, 0) #averaging by time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef64ff-a6a9-4d79-a971-1a254e9513be",
   "metadata": {},
   "source": [
    "This attention type is some averaging process, but very inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26a6c6c-a2b5-4384-aed1-30585e4a38d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.471403Z",
     "iopub.status.busy": "2025-03-29T10:02:07.471403Z",
     "iopub.status.idle": "2025-03-29T10:02:07.477411Z",
     "shell.execute_reply": "2025-03-29T10:02:07.476943Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.471403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d233d9f9-5885-4195-9937-8d398a5c0891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.477411Z",
     "iopub.status.busy": "2025-03-29T10:02:07.477411Z",
     "iopub.status.idle": "2025-03-29T10:02:07.482588Z",
     "shell.execute_reply": "2025-03-29T10:02:07.482588Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.477411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "542603bc-fa3f-4ac8-85be-332e658afe68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.483896Z",
     "iopub.status.busy": "2025-03-29T10:02:07.482588Z",
     "iopub.status.idle": "2025-03-29T10:02:07.487601Z",
     "shell.execute_reply": "2025-03-29T10:02:07.487601Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.483896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Recreating averaging using matrices\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim = True)\n",
    "\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) -> (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c053567-5a7c-40a1-a2ac-6c6a5409c319",
   "metadata": {},
   "source": [
    "wei shape is (T, T) @ and x shape is (B, T, C) which are inconsistent with each other. To conform with batch dimension torch will automatically create a batch dimension for wei tensor to perform batch multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82d70bd6-542b-4ab0-9ee0-99e7fde2856c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.487601Z",
     "iopub.status.busy": "2025-03-29T10:02:07.487601Z",
     "iopub.status.idle": "2025-03-29T10:02:07.492470Z",
     "shell.execute_reply": "2025-03-29T10:02:07.492470Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.487601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f196e589-dd6a-4edb-a0b6-61904e0e3029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.494089Z",
     "iopub.status.busy": "2025-03-29T10:02:07.492470Z",
     "iopub.status.idle": "2025-03-29T10:02:07.498237Z",
     "shell.execute_reply": "2025-03-29T10:02:07.498237Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.494089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6903b9ab-f9b3-4a67-a2d7-355d1db424ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.499455Z",
     "iopub.status.busy": "2025-03-29T10:02:07.499455Z",
     "iopub.status.idle": "2025-03-29T10:02:07.504037Z",
     "shell.execute_reply": "2025-03-29T10:02:07.503395Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.499455Z"
    }
   },
   "outputs": [],
   "source": [
    "# third version with softmax\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "\n",
    "# transform zero elements to negative infinity - to limit token's abilities to communicate with earlier tokens\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d451ab9c-e6e2-46e4-89fa-afdefb35f78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.504037Z",
     "iopub.status.busy": "2025-03-29T10:02:07.504037Z",
     "iopub.status.idle": "2025-03-29T10:02:07.509054Z",
     "shell.execute_reply": "2025-03-29T10:02:07.509054Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.504037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a6f2c42-d75e-4537-a409-9fd8b633475f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.510754Z",
     "iopub.status.busy": "2025-03-29T10:02:07.509054Z",
     "iopub.status.idle": "2025-03-29T10:02:07.513171Z",
     "shell.execute_reply": "2025-03-29T10:02:07.513171Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.510754Z"
    }
   },
   "outputs": [],
   "source": [
    "wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "xbow3 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33496675-bf12-4d06-9ed0-bdd8e47faf23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:02:07.514917Z",
     "iopub.status.busy": "2025-03-29T10:02:07.513171Z",
     "iopub.status.idle": "2025-03-29T10:02:07.520328Z",
     "shell.execute_reply": "2025-03-29T10:02:07.520328Z",
     "shell.execute_reply.started": "2025-03-29T10:02:07.514917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64367700-fce6-4495-9c21-86149083c065",
   "metadata": {},
   "source": [
    "## Simple Bi-Gram Language model\n",
    "\n",
    "Bigram language model, that only works with the last prediction of the model (context is only the one preceding token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d96a1eb-ae43-4958-85c6-e3100ad1cd6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:08:00.649827Z",
     "iopub.status.busy": "2025-03-29T10:08:00.648819Z",
     "iopub.status.idle": "2025-03-29T10:08:00.654956Z",
     "shell.execute_reply": "2025-03-29T10:08:00.654956Z",
     "shell.execute_reply.started": "2025-03-29T10:08:00.649827Z"
    }
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets = None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # output (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        logits = self.lm_head(x) # output (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None  \n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) #Tensor reshape\n",
    "            targets = targets.view(B*T) # or .view(-1)\n",
    "            # Loss expects logits in another shape, rather than (B, T, C) -> (B, C, T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            logits, loss = self(idx) \n",
    "\n",
    "            # Use only the last timestep - The main thing is that it's a Bi-Gram model, that's looking for the last timestep\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            # Apply softmax to use probabilities\n",
    "            probs = F.softmax(logits, dim = -1) # (B, C)\n",
    "            \n",
    "            # Sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "\n",
    "            # Append sampled index to te running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
    "        # Generate outputs as (B, T+max_new_tokens)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3a67cec-deb2-4882-a3f7-3827d821cdeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T08:53:40.682177Z",
     "iopub.status.busy": "2025-03-29T08:53:40.681107Z",
     "iopub.status.idle": "2025-03-29T08:53:40.754488Z",
     "shell.execute_reply": "2025-03-29T08:53:40.754488Z",
     "shell.execute_reply.started": "2025-03-29T08:53:40.682177Z"
    }
   },
   "outputs": [],
   "source": [
    "m = BigramLanguageModel(vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69f043be-b352-4560-b10e-f44cbd89f720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T08:53:40.754488Z",
     "iopub.status.busy": "2025-03-29T08:53:40.754488Z",
     "iopub.status.idle": "2025-03-29T08:53:40.823169Z",
     "shell.execute_reply": "2025-03-29T08:53:40.823169Z",
     "shell.execute_reply.started": "2025-03-29T08:53:40.754488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0364, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "937a52fa-d68c-4864-a663-dcaad1394f6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T08:53:40.824120Z",
     "iopub.status.busy": "2025-03-29T08:53:40.824120Z",
     "iopub.status.idle": "2025-03-29T08:53:40.891560Z",
     "shell.execute_reply": "2025-03-29T08:53:40.891560Z",
     "shell.execute_reply.started": "2025-03-29T08:53:40.824120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'y', 'q', '$', ';', 't', 'f', 'B', 'f', 'R', 'O', 'k', 'N', 'd', 'c', 'u', 'w', 'd', 'Z', 'Z', 'T', 'k', 'O', 'M', 'l', ';', ',', 'e', 'r', 't', 'K', '\\n', 'w', ':', '!', 'P', 'L', 'C', 'k', 'M', 'B', 'b', 'e', 'A', '$', '3', ':', 'X', 'a', 'S', 'G', 'J', 'O', '-', '3', 'p', '&', 'M', '-', 'c', '?', 'K', 'L', '3', 'a', 'u', 'h', 'p', 'F', 'Y', 'V', 'X', 'J', 'F', 'h', 'N', 'N', 'N', 'u', 'h', 'q', '$', 'O', 'M', 'x', 'v', '.', 't', 'b', 'V', 'F', 'Y', 'd', 'X', 'l', 'r', 'F', 'Z', 'a', 'A', 'e']\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device = device)\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f51787-02d9-4880-a244-168a2813d296",
   "metadata": {},
   "source": [
    "Train the model in order to increase it's efficiency and stability in text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92ccba10-365e-45d3-b31a-5b9f5d3fb99f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T08:53:40.891560Z",
     "iopub.status.busy": "2025-03-29T08:53:40.891560Z",
     "iopub.status.idle": "2025-03-29T08:53:42.057319Z",
     "shell.execute_reply": "2025-03-29T08:53:42.057319Z",
     "shell.execute_reply.started": "2025-03-29T08:53:40.891560Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cfff03f-1fe7-437e-9340-9baefea2982a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T08:53:42.057319Z",
     "iopub.status.busy": "2025-03-29T08:53:42.057319Z",
     "iopub.status.idle": "2025-03-29T08:53:55.058979Z",
     "shell.execute_reply": "2025-03-29T08:53:55.058979Z",
     "shell.execute_reply.started": "2025-03-29T08:53:42.057319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4487955570220947\n"
     ]
    }
   ],
   "source": [
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "995386c3-d438-4ad3-bea8-89e03dd833a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T08:53:55.060405Z",
     "iopub.status.busy": "2025-03-29T08:53:55.058979Z",
     "iopub.status.idle": "2025-03-29T08:53:55.229590Z",
     "shell.execute_reply": "2025-03-29T08:53:55.229590Z",
     "shell.execute_reply.started": "2025-03-29T08:53:55.060405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wawice my.\n",
      "\n",
      "HDEdarom oroup\n",
      "Yowhthetof isth ble mil; dill, ath iree sengmin lat Heriliovets, and Win nghir.\n",
      "Thanousel lind me l.\n",
      "HAshe ce hiry ptupr aisspllw y.\n",
      "Hurindu n Boopetelaves\n",
      "MPORDis, d mothakleo Windo whthCoribyo the m dourive we higend t so mower; te\n",
      "\n",
      "AN ad nterupt f s ar igr t m:\n",
      "\n",
      "Thiny aleronth,\n",
      "Mad\n",
      "RD:\n",
      "\n",
      "WISo myr f-NLIERor,\n",
      "Sb&hak\n",
      "Sadsal thes ghesthidin cour ay aney Iry ts I fr y ce.\n",
      "Jken pand, bemary.\n",
      "Yor 'Wour menm sora anghy t-senomes twe ten.\n",
      "Wand thot sulin s th llety ome.\n",
      "I muc\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device = device)\n",
    "print(''.join(decode(m.generate(context, max_new_tokens=500)[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b5bb1a-0d86-48de-ac3d-177b6ab7c135",
   "metadata": {},
   "source": [
    "We've got a simple Bi-Gram model, that was looking only to last token from predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "351df774-4cc2-4edb-8e54-a0b7ed82141f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T11:39:52.661397Z",
     "iopub.status.busy": "2025-03-29T11:39:52.660244Z",
     "iopub.status.idle": "2025-03-29T11:39:52.668381Z",
     "shell.execute_reply": "2025-03-29T11:39:52.668381Z",
     "shell.execute_reply.started": "2025-03-29T11:39:52.661397Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters):\n",
    "    '''\n",
    "        Estimate loss outputs more stable loss metrics \n",
    "    due to averaging calculated loss by number of batches.\n",
    "    '''\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7bea3c-2da4-4dc2-9495-d0c43bbe6452",
   "metadata": {},
   "source": [
    "# Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7d5e6-6422-4f28-9ace-09c5cca03192",
   "metadata": {},
   "source": [
    "In this implementation `wei` is a simple matrix with uniformly distributed weights, but attention necessarily needs more complicated path in order tokens to communicate with each other. It's necessary that `wei` matrix needs to be data dependent in order to work with sequences and attend to different parts of it.\n",
    "\n",
    "**Self-attention** solves this problem by emitting multiple vectors: **query**, **key** vectors.\n",
    "\n",
    "- **Query** - what information is being searched;\n",
    "- **Key** - what information is contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7928c308-63e5-4625-a9a6-0a9bb2072c75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:48:20.475299Z",
     "iopub.status.busy": "2025-03-29T10:48:20.474296Z",
     "iopub.status.idle": "2025-03-29T10:48:20.490275Z",
     "shell.execute_reply": "2025-03-29T10:48:20.489760Z",
     "shell.execute_reply.started": "2025-03-29T10:48:20.475299Z"
    }
   },
   "outputs": [],
   "source": [
    "# fourth version: self-attention\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "\n",
    "x = torch.randn(B,T,C) # Input tensor\n",
    "\n",
    "\n",
    "# implementing head of attention\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias = False) #Wk\n",
    "query = nn.Linear(C, head_size, bias = False) #Wq\n",
    "value = nn.Linear(C, head_size, bias = False) #Wv\n",
    "# K, Q matrices are created by the following steps\n",
    "K = key(x) # (B, T, 16) \n",
    "Q = query(x) # (B, T, 16)\n",
    "V = value(x) # (B, T, 16)\n",
    "\n",
    "wei = Q @ K.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "out = wei @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a4e2d-9c9e-4f87-a800-73ba40528e00",
   "metadata": {},
   "source": [
    "Projections made by `Wk` and `Wq` matrices of `query` and `keys` vectors have created `Key` and `Query` matrices which further were matrix multiplied to create a matrix of dot-products that indicates affinity scores between *keys* and *queries* (the higher the score is the more important is one token to another).\n",
    "\n",
    "These lines of code:\n",
    "\n",
    "```\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim = -1)\n",
    "```\n",
    "help to turn on and off some attributes of attention mechanism:\n",
    "\n",
    "1. `wei.masked_fill(tril == 0, float('-inf'))` helps to turn on and off further context for each token;\n",
    "2. `F.softmax(wei, dim = -1)` creates normalized data distribution for efficient and interpretable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d38f82cd-6c03-40c8-9f9d-853abe92623c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:48:37.173193Z",
     "iopub.status.busy": "2025-03-29T10:48:37.171686Z",
     "iopub.status.idle": "2025-03-29T10:48:37.178528Z",
     "shell.execute_reply": "2025-03-29T10:48:37.178528Z",
     "shell.execute_reply.started": "2025-03-29T10:48:37.173193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5249423-e382-4c70-a2fb-bc6344fede98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:48:38.027989Z",
     "iopub.status.busy": "2025-03-29T10:48:38.027989Z",
     "iopub.status.idle": "2025-03-29T10:48:38.033856Z",
     "shell.execute_reply": "2025-03-29T10:48:38.033856Z",
     "shell.execute_reply.started": "2025-03-29T10:48:38.027989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0a9a2-7bdd-455e-89da-78df6f20d192",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e8c869e-7de4-4ad3-bb13-d27569c240bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T11:15:56.649035Z",
     "iopub.status.busy": "2025-03-29T11:15:56.649035Z",
     "iopub.status.idle": "2025-03-29T11:15:56.657128Z",
     "shell.execute_reply": "2025-03-29T11:15:56.657128Z",
     "shell.execute_reply.started": "2025-03-29T11:15:56.649035Z"
    }
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, n_emb, head_size:int = 16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wq = nn.Linear(n_emb, head_size, bias = False)\n",
    "        self.Wk = nn.Linear(n_emb, head_size, bias = False)\n",
    "        self.Wv = nn.Linear(n_emb, head_size, bias = False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        Q = self.Wq(x) # (B, T, C)\n",
    "        K = self.Wk(x) # (B, T, C)\n",
    "        V = self.Wv(x) # (B, T, C)\n",
    "\n",
    "        # Attention compute + normalization = softmax(QK/sqrt(head_dim))*Value\n",
    "        wei = Q @ K.transpose(-2, -1) * (C ** -0.5) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) - decoder block\n",
    "        wei = F.softmax(wei, dim = -1) # (B, T, T)\n",
    "\n",
    "        # Compute attention - weighted aggregates (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        out = wei @ V \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b209512-f807-47f1-a788-57d2ccbcb6ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T11:42:20.647736Z",
     "iopub.status.busy": "2025-03-29T11:42:20.645753Z",
     "iopub.status.idle": "2025-03-29T11:42:20.654570Z",
     "shell.execute_reply": "2025-03-29T11:42:20.654570Z",
     "shell.execute_reply.started": "2025-03-29T11:42:20.647736Z"
    }
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, n_embed, sa_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_head = Head(n_embed, sa_dim)\n",
    "        self.lm_head = nn.Linear(sa_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets = None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # output (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.sa_head(x) # apply one head of self-attention\n",
    "        logits = self.lm_head(x) # output (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None  \n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) #Tensor reshape\n",
    "            targets = targets.view(B*T) # or .view(-1)\n",
    "            # Loss expects logits in another shape, rather than (B, T, C) -> (B, C, T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    " \n",
    "            idx_cond = idx[:, -block_size:] # Crop the context to cpmly with positional embeddings\n",
    "            \n",
    "            logits, loss = self(idx_cond) \n",
    "\n",
    "            # Use only the last timestep - The main thing is that it's a Bi-Gram model, that's looking for the last timestep\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            # Apply softmax to use probabilities\n",
    "            probs = F.softmax(logits, dim = -1) # (B, C)\n",
    "            \n",
    "            # Sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "\n",
    "            # Append sampled index to te running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
    "        # Generate outputs as (B, T+max_new_tokens)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3da45c95-4bac-4f1f-b998-8434ab083a35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T11:42:43.342543Z",
     "iopub.status.busy": "2025-03-29T11:42:43.340538Z",
     "iopub.status.idle": "2025-03-29T11:42:43.348586Z",
     "shell.execute_reply": "2025-03-29T11:42:43.346941Z",
     "shell.execute_reply.started": "2025-03-29T11:42:43.342543Z"
    }
   },
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 10000\n",
    "eval_interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "39b861f3-01ba-4c13-bb8b-a35a6d6bc534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T11:42:44.589861Z",
     "iopub.status.busy": "2025-03-29T11:42:44.589861Z",
     "iopub.status.idle": "2025-03-29T11:43:16.173557Z",
     "shell.execute_reply": "2025-03-29T11:43:16.172669Z",
     "shell.execute_reply.started": "2025-03-29T11:42:44.589861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004977 M parameters\n",
      "step 0: train loss 4.2726, val loss 4.2596\n",
      "step 500: train loss 3.1551, val loss 3.0472\n",
      "1000: train loss 2.9469, val loss 2.9577\n",
      "step 1500: train loss 2.8796, val loss 2.8891\n",
      "2000: train loss 2.7680, val loss 2.6833\n",
      "\n",
      "step 2500: train loss 2.6448, val loss 2.6929\n",
      "3000: train loss 2.5788, val loss 2.5944\n",
      "\n",
      "step 3500: train loss 2.5840, val loss 2.6112\n",
      "4000: train loss 2.6323, val loss 2.5533\n",
      "\n",
      "step 4500: train loss 2.4967, val loss 2.5709\n",
      "5000: train loss 2.4904, val loss 2.4454\n",
      "\n",
      "step 5500: train loss 2.4253, val loss 2.5030\n",
      "6000: train loss 2.4442, val loss 2.5205\n",
      "\n",
      "step 6500: train loss 2.4345, val loss 2.5112\n",
      "7000: train loss 2.4360, val loss 2.4100\n",
      "\n",
      "step 7500: train loss 2.5096, val loss 2.5521\n",
      "8000: train loss 2.4538, val loss 2.4576\n",
      "\n",
      "step 8500: train loss 2.5051, val loss 2.3144\n",
      "9000: train loss 2.4416, val loss 2.4762\n",
      "\n",
      "step 9500: train loss 2.4685, val loss 2.4203\n",
      "999: train loss 2.4579, val loss 2.5704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size, block_size, 32, 16)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(m, 20)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "21cf6ecd-3994-4dca-9d94-e8f5d0f34699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T11:43:30.098593Z",
     "iopub.status.busy": "2025-03-29T11:43:30.098593Z",
     "iopub.status.idle": "2025-03-29T11:43:32.166491Z",
     "shell.execute_reply": "2025-03-29T11:43:32.166491Z",
     "shell.execute_reply.started": "2025-03-29T11:43:30.098593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B; emlant.\n",
      "\n",
      "3 thule bilomy thanve cstinoliren.\n",
      "GI mo ngo t fas; th ine,\n",
      "Ave twhte ithe\n",
      "Bur cthurld uthise,\n",
      "Nkaave I't harl Se flo picuen, ad am nd was\n",
      "re ofouse sis is it LHire\n",
      "Th!\n",
      "O: in be d ym owthand iy cour th mance rl thith thee ther thamor, heren ndurencs! adyong llima unt whs ythelased pille adriey meawce ainon maprotu ouril ladd nt y. TI mireve th bst ar my pas, th the. he vous.\n",
      "\n",
      "\n",
      "ERCNENLANUEOS bur elanno shof maroll cs ake thirawsereeait ty, yourr hpr dk sl su nfud\n",
      "As mowto quu ceman omefipel, lind th brath, to yacust, he theighaig tre meghor A.\n",
      "Ad.\n",
      "\n",
      "LANADdr enver oll bnod.\n",
      "\n",
      "Tind RI IKIDog'd th Bvy to, w:ofo wre thandr ngoru Y m.\n",
      "\n",
      "Merrd.\n",
      "\n",
      "S otesl pr fithere p dth me we weirg onecloil ivanfut; tham cet ier,\n",
      "Thes me yo hrine kars ksste,\n",
      "This, thecodunickit whis,\n",
      "ALAre buth ghanto whatant woan nBom tghayerm aikn blaithe.\n",
      "\n",
      "I, tishegun inde nokere prom ben cay t.\n",
      "ASouy corimang, husto ot thadre;\n",
      "PHin! hy theg win criisen?\n",
      "\n",
      "Bek'lod to yr ding hs teat jrde bre' thes I: weay wong ispthr's chas buem hiefrs ft thwelim the'set,\n",
      "I kquink wheren stinco in ksig ars th iels:\n",
      "K:\n",
      "LA IU:\n",
      "al:\n",
      "Seare whe afrut villst, ort?\n",
      "\n",
      "RERTERNGGLand pa thet try htichage hamryo isk\n",
      "Fe, bem alllrig nt, bes ilon alThans sl I okem, whed Th yud?\n",
      "\n",
      "Moourgie herer cachany hu rance th, mearseanw\n",
      "We thepou itis len the wst lorow hey meve.\n",
      ": te ies.\n",
      "\n",
      "Hofry thipte at str'l tht wh tha wh yitoer praman tar PAgchyoou He che toooone pianenen thance anatho' apile wod the ot cagat xpo to ing thanl ow bus lid!\n",
      "\n",
      "He t dly nbeno ndr,\n",
      "loieante fe, ors iy bnous he thgadd vderesithacngl gake Lof th I sar Tham th,\n",
      "Sou the bo akeacthivel my nevet gn chy lt it berat therie ce clout kame Handurped wany rviary.\n",
      "\n",
      "Thit s.\n",
      "\n",
      "So chafy ny POHair'th ow maaasu to scun th cot her tho odo, ak che ss tre te been st thes, GI rf; jeo nwo, th qpttipours naf!\n",
      "Agunce!\n",
      "\n",
      "MINGY:\n",
      "Ses nws.\n",
      "\n",
      "KNierry hy hat; tun Whhoovengs thethe INiked tho, hede th yfoudis' lars st wit y-se burll ienil omuraspaik me wr,\n",
      "Wofes sn Sonme at d\n",
      "wigh;\n",
      "Son ced yow\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(''.join(decode(m.generate(context, max_new_tokens=2000)[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d06136-4f32-4926-a0c7-f331224a0b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
