{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80035271-b19b-432c-a482-6db40fa86fb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-01T09:27:36.709028Z",
     "iopub.status.busy": "2025-05-01T09:27:36.709028Z",
     "iopub.status.idle": "2025-05-01T09:27:46.653669Z",
     "shell.execute_reply": "2025-05-01T09:27:46.651627Z",
     "shell.execute_reply.started": "2025-05-01T09:27:36.709028Z"
    },
    "id": "80035271-b19b-432c-a482-6db40fa86fb9",
    "outputId": "b0950b44-b51e-4cc5-f3a0-9d62a7da06b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8644c-c5b4-4d95-8063-0b44b63e8708",
   "metadata": {
    "id": "5fd8644c-c5b4-4d95-8063-0b44b63e8708"
   },
   "source": [
    "# Build your LLM from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffcf33d-6874-417f-9252-bfb8b104b9d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:27:46.655680Z",
     "iopub.status.busy": "2025-05-01T09:27:46.655680Z",
     "iopub.status.idle": "2025-05-01T09:28:07.327199Z",
     "shell.execute_reply": "2025-05-01T09:28:07.327199Z",
     "shell.execute_reply.started": "2025-05-01T09:27:46.655680Z"
    },
    "id": "1ffcf33d-6874-417f-9252-bfb8b104b9d0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624cec2c-8540-44ab-974d-e20281e19c51",
   "metadata": {
    "id": "624cec2c-8540-44ab-974d-e20281e19c51"
   },
   "source": [
    "# Working with text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4ed2e1-33d4-4ba3-9302-27010ddeffaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:07.331336Z",
     "iopub.status.busy": "2025-05-01T09:28:07.327199Z",
     "iopub.status.idle": "2025-05-01T09:28:07.920848Z",
     "shell.execute_reply": "2025-05-01T09:28:07.920848Z",
     "shell.execute_reply.started": "2025-05-01T09:28:07.331336Z"
    },
    "id": "af4ed2e1-33d4-4ba3-9302-27010ddeffaf"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ecddc3e-1735-4142-8597-6663e709d1e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:07.930220Z",
     "iopub.status.busy": "2025-05-01T09:28:07.924801Z",
     "iopub.status.idle": "2025-05-01T09:28:07.969704Z",
     "shell.execute_reply": "2025-05-01T09:28:07.969704Z",
     "shell.execute_reply.started": "2025-05-01T09:28:07.930220Z"
    },
    "id": "2ecddc3e-1735-4142-8597-6663e709d1e5",
    "outputId": "1fb873b1-2e46-4226-f2d9-55cc914b8d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"data\", \"the-verdict.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6603597c-4442-49a2-a183-c3c2baede0ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:07.973904Z",
     "iopub.status.busy": "2025-05-01T09:28:07.969704Z",
     "iopub.status.idle": "2025-05-01T09:28:07.993350Z",
     "shell.execute_reply": "2025-05-01T09:28:07.993350Z",
     "shell.execute_reply.started": "2025-05-01T09:28:07.973904Z"
    },
    "id": "6603597c-4442-49a2-a183-c3c2baede0ce",
    "outputId": "607a78c4-3b0d-467a-9ea3-da9376efca90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# Length of a sample text tokenized by GPT-2 BPE tokenizer\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd1c22-6716-4bcb-9162-efd17200aaf7",
   "metadata": {
    "id": "bbcd1c22-6716-4bcb-9162-efd17200aaf7"
   },
   "source": [
    "## Data sampling with a sliding window approach\n",
    "\n",
    "Using torch's `Dataset` class to create a simple textual dataset for GPT training. The code uses sliding window approach (combined with additional strides) to create a new dataset.\n",
    "\n",
    "![sliding_window](https://camo.githubusercontent.com/9c738e75095f70d3dc4f6b3630008dd67607b5fa92e3bf776b0ed2cbb68db299/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31332e776562703f313233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ac7021-b079-4d16-b98f-8fc9e0c1f973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:07.996337Z",
     "iopub.status.busy": "2025-05-01T09:28:07.996337Z",
     "iopub.status.idle": "2025-05-01T09:28:08.010042Z",
     "shell.execute_reply": "2025-05-01T09:28:08.010042Z",
     "shell.execute_reply.started": "2025-05-01T09:28:07.996337Z"
    },
    "id": "a3ac7021-b079-4d16-b98f-8fc9e0c1f973"
   },
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length: int = 256, stride: int = 0) -> Dataset:\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt) # tokenize entire text\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7610cf45-d225-48a5-89a2-3ff1c8911ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.014983Z",
     "iopub.status.busy": "2025-05-01T09:28:08.012972Z",
     "iopub.status.idle": "2025-05-01T09:28:08.029757Z",
     "shell.execute_reply": "2025-05-01T09:28:08.028260Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.014983Z"
    },
    "id": "7610cf45-d225-48a5-89a2-3ff1c8911ae4"
   },
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size: int = 4, max_length: int = 256,\n",
    "                         stride: int = 128, shuffle = True, drop_last = True,\n",
    "                         num_workers: int = 0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d385bc04-8fea-450e-8556-365ef80e0a3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.037888Z",
     "iopub.status.busy": "2025-05-01T09:28:08.029757Z",
     "iopub.status.idle": "2025-05-01T09:28:08.334874Z",
     "shell.execute_reply": "2025-05-01T09:28:08.334874Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.029757Z"
    },
    "id": "d385bc04-8fea-450e-8556-365ef80e0a3e",
    "outputId": "ff95d4ec-804d-4fdf-b8f1-100106d8b354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3091e-930a-488e-9687-a634b3fe378c",
   "metadata": {
    "id": "7ff3091e-930a-488e-9687-a634b3fe378c"
   },
   "source": [
    "Creating embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54d40488-ceff-44aa-b951-71b82c504b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.340859Z",
     "iopub.status.busy": "2025-05-01T09:28:08.334874Z",
     "iopub.status.idle": "2025-05-01T09:28:08.568604Z",
     "shell.execute_reply": "2025-05-01T09:28:08.566573Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.340859Z"
    },
    "id": "54d40488-ceff-44aa-b951-71b82c504b14"
   },
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "max_length = 4\n",
    "context_length = max_length\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dca1a7-a73c-4fe0-9ded-1134097d959d",
   "metadata": {
    "id": "a5dca1a7-a73c-4fe0-9ded-1134097d959d"
   },
   "source": [
    "Using embedding layers the procedure is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ced0d52f-b4dd-4edc-8f5f-6ccfe274a215",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.580475Z",
     "iopub.status.busy": "2025-05-01T09:28:08.577878Z",
     "iopub.status.idle": "2025-05-01T09:28:08.668524Z",
     "shell.execute_reply": "2025-05-01T09:28:08.668524Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.580475Z"
    },
    "id": "ced0d52f-b4dd-4edc-8f5f-6ccfe274a215",
    "outputId": "e9c21ce6-42ac-4332-be3c-5f8ba93734e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID's tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Shape torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token ID's\", inputs)\n",
    "print(\"Shape\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acecfa3e-8d23-4a2e-8960-f7acd7ad22b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.668524Z",
     "iopub.status.busy": "2025-05-01T09:28:08.668524Z",
     "iopub.status.idle": "2025-05-01T09:28:08.706799Z",
     "shell.execute_reply": "2025-05-01T09:28:08.706209Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.668524Z"
    },
    "id": "acecfa3e-8d23-4a2e-8960-f7acd7ad22b1",
    "outputId": "9dc84976-939d-44b6-faab-c154b39ad714"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedded tokens\n",
    "token_embedding_layer(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ce1cc5-4ad6-49de-aaeb-b3b831314c7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.710905Z",
     "iopub.status.busy": "2025-05-01T09:28:08.710016Z",
     "iopub.status.idle": "2025-05-01T09:28:08.730433Z",
     "shell.execute_reply": "2025-05-01T09:28:08.730433Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.710905Z"
    },
    "id": "23ce1cc5-4ad6-49de-aaeb-b3b831314c7e",
    "outputId": "cc870f29-8de7-442d-c9ed-397849909b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Absolute embeddings for positional encoding are created using the following procedure\n",
    "\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63393840-bab6-441d-a4c4-dadc696de619",
   "metadata": {
    "id": "63393840-bab6-441d-a4c4-dadc696de619"
   },
   "source": [
    "Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model:\n",
    "![pos_embddings](https://camo.githubusercontent.com/e53fdceda6a07218acfe115d81dc930241569fbbcd5c3e533856dee1959a8a93/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31382e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd8163ee-86fd-4852-8787-5a25e1aa1d23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.735239Z",
     "iopub.status.busy": "2025-05-01T09:28:08.735239Z",
     "iopub.status.idle": "2025-05-01T09:28:08.749190Z",
     "shell.execute_reply": "2025-05-01T09:28:08.748347Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.735239Z"
    },
    "id": "fd8163ee-86fd-4852-8787-5a25e1aa1d23",
    "outputId": "9cf8be41-f3e0-4689-cea5-7278941a1db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embedding_layer(inputs) + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c87d4-d4e1-4f0b-80dd-b38ee96afb42",
   "metadata": {
    "id": "ac4c87d4-d4e1-4f0b-80dd-b38ee96afb42"
   },
   "source": [
    "# Attention implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d73ac9-746e-4faf-9cfd-2d735855d3ad",
   "metadata": {
    "id": "91d73ac9-746e-4faf-9cfd-2d735855d3ad"
   },
   "source": [
    "Simple attention idea is the following: context-aware vector the represents attention mechanism is calculated for each token in an input sequence in order to create vector representations that contain the most valuable information for each part of the input sequence.\n",
    "\n",
    "## Simple attention\n",
    "\n",
    "Simple self-attention implements an immutable attention mechanism based solely on matrix multiplication. Attention calulation is perfomed in the following way:\n",
    "1. For each `Query` vector compute dot products with each `Key` vectors;\n",
    "2. Transform results of dot products using *softmax* function in order to normalize attention multipliers (**interpretation:** probabilities of relative importance for each token);\n",
    "3. Compute attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66a1420e-94fb-47f5-8eaa-60a1397b5690",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.749190Z",
     "iopub.status.busy": "2025-05-01T09:28:08.749190Z",
     "iopub.status.idle": "2025-05-01T09:28:08.811351Z",
     "shell.execute_reply": "2025-05-01T09:28:08.811351Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.749190Z"
    },
    "id": "66a1420e-94fb-47f5-8eaa-60a1397b5690",
    "outputId": "990a1ea2-4036-4c3a-f5d4-092665475d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423418d-f362-47aa-845c-021040269af5",
   "metadata": {
    "id": "c423418d-f362-47aa-845c-021040269af5"
   },
   "source": [
    "Inputs serve as Q, K, V vectors at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20dee42f-e03d-4866-929d-e1426365a1c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.816231Z",
     "iopub.status.busy": "2025-05-01T09:28:08.814216Z",
     "iopub.status.idle": "2025-05-01T09:28:08.876096Z",
     "shell.execute_reply": "2025-05-01T09:28:08.875586Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.814216Z"
    },
    "id": "20dee42f-e03d-4866-929d-e1426365a1c0",
    "outputId": "9b806fd9-3468-4766-c713-03c5dadc38f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "Attention weights\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "Attention weights check\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print('Attention scores', attn_scores, sep = '\\n')\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores, dim = -1)\n",
    "print('Attention weights', attn_weights, sep = '\\n')\n",
    "print('Attention weights check', attn_weights.sum(dim = -1), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dd7ae95-a801-4701-9d28-117e8cdb4071",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.877941Z",
     "iopub.status.busy": "2025-05-01T09:28:08.877941Z",
     "iopub.status.idle": "2025-05-01T09:28:08.890349Z",
     "shell.execute_reply": "2025-05-01T09:28:08.890349Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.877941Z"
    },
    "id": "1dd7ae95-a801-4701-9d28-117e8cdb4071",
    "outputId": "9e2e602a-a758-4be5-b009-fc4a61acf7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final context vectors\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "context_vectors = attn_weights @ inputs\n",
    "print('Final context vectors', context_vectors, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b7b34-9ab3-4398-a160-abf3b21d5726",
   "metadata": {
    "id": "507b7b34-9ab3-4398-a160-abf3b21d5726"
   },
   "source": [
    "## Trainable self-attention\n",
    "\n",
    "Self-attention mechanism that is so-called **Scaled Dot-Product Attention**. This attention mechanism is trainable and can perform necessary update due to use of $W_q; W_k; W_v$ matrices that represent *Query, Key, Value* weight matrices.\n",
    "\n",
    "**Attention scores** are dynamically updated coefficients that change due to input information mutations, however **QKV** weight matrices are static matrices that update only while training.\n",
    "$$\\text{Attention} = \\text{softmax} \\bigl( \\frac{Q \\cdot K}{\\sqrt{d_{keys}}}\\bigr) V$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a43c14b-5e34-46b7-8df0-fe3e600fe1c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.893201Z",
     "iopub.status.busy": "2025-05-01T09:28:08.893201Z",
     "iopub.status.idle": "2025-05-01T09:28:08.901844Z",
     "shell.execute_reply": "2025-05-01T09:28:08.899675Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.893201Z"
    },
    "id": "3a43c14b-5e34-46b7-8df0-fe3e600fe1c3"
   },
   "outputs": [],
   "source": [
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2e5ffea-c614-4fe6-a74b-2498828fa97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.902875Z",
     "iopub.status.busy": "2025-05-01T09:28:08.902875Z",
     "iopub.status.idle": "2025-05-01T09:28:08.918257Z",
     "shell.execute_reply": "2025-05-01T09:28:08.918257Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.902875Z"
    },
    "id": "f2e5ffea-c614-4fe6-a74b-2498828fa97b"
   },
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out)) # (emb_size, attn_emb_size)\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out)) # (emb_size, attn_emb_size)\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out)) # (emb_size, attn_emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query # (n_tokens, emb_size) X (emb_size, attn_emb_size) -> (n_tokens, attn_emb_size)\n",
    "        keys = x @ self.W_key # (n_tokens, emb_size) X (emb_size, attn_emb_size) -> (n_tokens, attn_emb_size)\n",
    "        values = x @ self.W_value # (n_tokens, emb_size) X (emb_size, attn_emb_size) -> (n_tokens, attn_emb_size)\n",
    "\n",
    "        attn_scores = queries @ keys.T # (n_tokens, attn_emb_size) X (attn_emb_size, n_tokens) -> (n_tokens, n_tokens)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim = -1\n",
    "        )\n",
    "        context_vectors = attn_weights @ values # (n_tokens, n_tokens) X (n_tokens, attn_emb_size) -> (n_tokens, attn_emb_size)\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dc56662-1f95-4145-af54-c1af6b264c8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:08.924308Z",
     "iopub.status.busy": "2025-05-01T09:28:08.924308Z",
     "iopub.status.idle": "2025-05-01T09:28:09.045762Z",
     "shell.execute_reply": "2025-05-01T09:28:09.044676Z",
     "shell.execute_reply.started": "2025-05-01T09:28:08.924308Z"
    },
    "id": "6dc56662-1f95-4145-af54-c1af6b264c8b",
    "outputId": "8ae75c2d-bff1-492f-8436-649ad2c81278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "\n",
    "# Calculate context vectors\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452be7a1-c18f-4c6f-90c7-95b3c0956b54",
   "metadata": {
    "id": "452be7a1-c18f-4c6f-90c7-95b3c0956b54"
   },
   "source": [
    "![image.png](https://camo.githubusercontent.com/5edb6b2e02db4ad16761a1c6a6de4f75b16d6d03e2eac6d52c263669ea358308/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830335f636f6d707265737365642f31382e77656270)\n",
    "\n",
    "Usage of `nn.Linear` layers is preferred in the self attention class due to a more efficient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53ecb3b0-9933-4d22-b3af-a40f6818365a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.050787Z",
     "iopub.status.busy": "2025-05-01T09:28:09.047761Z",
     "iopub.status.idle": "2025-05-01T09:28:09.067644Z",
     "shell.execute_reply": "2025-05-01T09:28:09.065629Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.050787Z"
    },
    "id": "53ecb3b0-9933-4d22-b3af-a40f6818365a"
   },
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, qkv_bias) # (emb_size, attn_emb_size)\n",
    "        self.W_key = nn.Linear(d_in, d_out, qkv_bias) # (emb_size, attn_emb_size)\n",
    "        self.W_value = nn.Linear(d_in, d_out, qkv_bias) # (emb_size, attn_emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x) # (n_tokens, emb_size) X (emb_size, attn_emb_size) -> (n_tokens, attn_emb_size)\n",
    "        keys = self.W_key(x) # (n_tokens, emb_size) X (emb_size, attn_emb_size) -> (n_tokens, attn_emb_size)\n",
    "        values = self.W_value(x) # (n_tokens, emb_size) X (emb_size, attn_emb_size) -> (n_tokens, attn_emb_size)\n",
    "\n",
    "        attn_scores = queries @ keys.T # (n_tokens, attn_emb_size) X (attn_emb_size, n_tokens) -> (n_tokens, n_tokens)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim = -1\n",
    "        )\n",
    "        context_vectors = attn_weights @ values # (n_tokens, n_tokens) X (n_tokens, attn_emb_size) -> (n_tokens, attn_emb_size)\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c9d0e54-df8f-4d4a-a704-8c156d00cd52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.069657Z",
     "iopub.status.busy": "2025-05-01T09:28:09.069657Z",
     "iopub.status.idle": "2025-05-01T09:28:09.095534Z",
     "shell.execute_reply": "2025-05-01T09:28:09.094625Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.069657Z"
    },
    "id": "6c9d0e54-df8f-4d4a-a704-8c156d00cd52",
    "outputId": "66669c92-6963-414c-c849-ebb085fcd43c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "# Calculate context vectors\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e1686-0598-4cef-93a2-0eec3ea0e85a",
   "metadata": {
    "id": "267e1686-0598-4cef-93a2-0eec3ea0e85a"
   },
   "source": [
    "## Causal attention\n",
    "\n",
    "Causal attention (*masked attention*) is used for restricting LM. *Masked attention* forces LM to consider only previous tokens when generating the output.\n",
    "\n",
    "**Causal attention masks future tokens.**\n",
    "![image.png](https://camo.githubusercontent.com/ae6a1857af914fbb7d57da177ce6bff4b57dbebe3bd11395855b3315ae13d1e1/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830335f636f6d707265737365642f31392e77656270)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q4FeLEN9m05S",
   "metadata": {
    "id": "Q4FeLEN9m05S"
   },
   "source": [
    "Normalization steps performed during attention calculation can cause information leakage during this procedure, however *properties* of the softmax function can nullify this error.\n",
    "\n",
    "*The mathematical elegance of softmax is that despite initially including all positions in the denominator, after masking and renormalizing, the effect of the masked positions is nullified—they don’t contribute to the softmax score in any meaningful way.*\n",
    "\n",
    "*In simpler terms, after masking and renormalization, the distribution of attention weights is as if it was calculated only among the unmasked positions to begin with. This ensures there’s no information leakage from future (or otherwise masked) tokens as we intended.*\n",
    "\n",
    "*Causal attention* can be improved using `inf` values that will faster the computations due to lesser number of operations in attention mechanism.\n",
    "\n",
    "![image](https://camo.githubusercontent.com/085c511ba76dafdd9bc7d9fc8e15fdf24770dfa7d8061cc2441675411051c5f0/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830335f636f6d707265737365642f32302e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccd695d3-ce86-4ba5-aaa6-3e3944c468df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.095534Z",
     "iopub.status.busy": "2025-05-01T09:28:09.095534Z",
     "iopub.status.idle": "2025-05-01T09:28:09.107399Z",
     "shell.execute_reply": "2025-05-01T09:28:09.107399Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.095534Z"
    },
    "id": "ccd695d3-ce86-4ba5-aaa6-3e3944c468df",
    "outputId": "766d3773-8c10-41ff-d8f6-d185c2459ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "B3OnVOeApV1s",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.110614Z",
     "iopub.status.busy": "2025-05-01T09:28:09.109772Z",
     "iopub.status.idle": "2025-05-01T09:28:09.125924Z",
     "shell.execute_reply": "2025-05-01T09:28:09.125924Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.110614Z"
    },
    "id": "B3OnVOeApV1s"
   },
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias: bool = False) -> None:\n",
    "      super().__init__()\n",
    "      self.W_query = nn.Linear(d_in, d_out, qkv_bias)\n",
    "      self.W_key = nn.Linear(d_in, d_out, qkv_bias)\n",
    "      self.W_value = nn.Linear(d_in, d_out, qkv_bias)\n",
    "\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "      # Transformers context length defines attention mask that can be applied to the data\n",
    "      # Create an upper triangular attention mask (mask is applied on a ones matrix) that is used for causal attention\n",
    "      self.register_buffer('mask', torch.triu(torch.ones((context_length, context_length)), diagonal = 1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch, num_tokens, d_in = x.shape # B, T, C\n",
    "    queries = self.W_query(x) # T, CS\n",
    "    keys = self.W_key(x) # T, CS\n",
    "    values = self.W_value(x) # T, CS\n",
    "\n",
    "    # Transpose keys matrices in each batch element\n",
    "    attn_scores = queries @ keys.transpose(1, 2) # T, T\n",
    "\n",
    "    # Omit scores that are not used in calculation\n",
    "    attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "\n",
    "    attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "    attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "    context_vectors = attn_weights @ values\n",
    "    return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "T0aor23DxIKT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.129000Z",
     "iopub.status.busy": "2025-05-01T09:28:09.129000Z",
     "iopub.status.idle": "2025-05-01T09:28:09.172700Z",
     "shell.execute_reply": "2025-05-01T09:28:09.172700Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.129000Z"
    },
    "id": "T0aor23DxIKT",
    "outputId": "ac1b0464-abb7-4a86-b58c-4948e77a9bc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "causal = CausalAttention(d_in, d_out, batch.shape[1], 0.1)\n",
    "\n",
    "# Calculate context vectors\n",
    "print(causal(batch).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FixTpF5czE4-",
   "metadata": {
    "id": "FixTpF5czE4-"
   },
   "source": [
    "## Multihead attention mechanism\n",
    "\n",
    "Multi-head attention is the form of causal attention that implements multiple independent attention heads that will analyze the sequence in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "CQfFtLCSy7VJ",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.180262Z",
     "iopub.status.busy": "2025-05-01T09:28:09.176723Z",
     "iopub.status.idle": "2025-05-01T09:28:09.197483Z",
     "shell.execute_reply": "2025-05-01T09:28:09.197033Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.180262Z"
    },
    "id": "CQfFtLCSy7VJ"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, d_in, d_out,\n",
    "               num_heads, context_length,\n",
    "               dropout, qkv_bias: bool = True):\n",
    "    super().__init__()\n",
    "\n",
    "    assert (d_out % num_heads == 0), 'SA Head embedding size must be divisible by number of heads'\n",
    "    self.d_out = d_out\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = d_out // num_heads\n",
    "    self.W_query = nn.Linear(d_in, d_out, qkv_bias)\n",
    "    self.W_key = nn.Linear(d_in, d_out, qkv_bias)\n",
    "    self.W_value = nn.Linear(d_in, d_out, qkv_bias)\n",
    "    self.out_proj = nn.Linear(d_out, d_out)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch, num_tokens, emb_size = x.shape\n",
    "\n",
    "    Q = self.W_query(x)\n",
    "    K = self.W_key(x)\n",
    "    V = self.W_value(x)\n",
    "\n",
    "\n",
    "    Q = Q.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "    K = K.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "    V = V.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "    Q = Q.transpose(1, 2)\n",
    "    K = K.transpose(1, 2)\n",
    "    V = V.transpose(1, 2)\n",
    "\n",
    "    attn_scores = Q @ K.transpose(2, 3)\n",
    "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "    # masked_fill_ (underscore) changes tensor inplace reducing unnecessary copying\n",
    "    attn_scores.masked_fill_(mask_bool, -torch.inf) \n",
    "\n",
    "    attn_weights = torch.softmax(attn_scores / K.shape[-1] ** 0.5, dim = -1)\n",
    "    attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "    context_vectors = (attn_weights @ V).transpose(1, 2)\n",
    "    context_vectors = context_vectors.contiguous().view(batch, num_tokens, self.d_out)\n",
    "\n",
    "    context_vectors = self.out_proj(context_vectors)    #11\n",
    "    return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bac56292-6424-4896-8d3c-1e8eb1758167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.197483Z",
     "iopub.status.busy": "2025-05-01T09:28:09.197483Z",
     "iopub.status.idle": "2025-05-01T09:28:09.225981Z",
     "shell.execute_reply": "2025-05-01T09:28:09.225676Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.197483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "heads = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, heads, batch.shape[1], 0.1)\n",
    "\n",
    "# Calculate context vectors\n",
    "print(mha(batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbd23893-d8e5-442a-ad00-b86e3e8e08a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.225981Z",
     "iopub.status.busy": "2025-05-01T09:28:09.225981Z",
     "iopub.status.idle": "2025-05-01T09:28:09.249044Z",
     "shell.execute_reply": "2025-05-01T09:28:09.249044Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.225981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7963, -0.1726],\n",
       "         [ 0.7935, -0.1266],\n",
       "         [ 0.7910, -0.1149],\n",
       "         [ 0.7689, -0.1457],\n",
       "         [ 0.7696, -0.1541],\n",
       "         [ 0.7314, -0.2289]],\n",
       "\n",
       "        [[ 0.7963, -0.1726],\n",
       "         [ 0.7935, -0.1266],\n",
       "         [ 0.7910, -0.1149],\n",
       "         [ 0.7689, -0.1457],\n",
       "         [ 0.7696, -0.1541],\n",
       "         [ 0.7625, -0.1589]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfecbbe-628d-4f6d-ba43-80decaaedd65",
   "metadata": {},
   "source": [
    "# Implementing GPT\n",
    "\n",
    "We use short variable names to avoid long lines of code later:\n",
    "- \"vocab_size\" indicates a vocabulary size of 50,257 words, supported by the BPE tokenizer;\n",
    "- \"context_length\" represents the model's maximum input token count, as enabled by positional embeddings;\n",
    "- \"emb_dim\" is the embedding size for token inputs, converting each input token into a 768-dimensional vector;\n",
    "- \"n_heads\" is the number of attention heads in the multi-head attention mechanism;\n",
    "- \"n_layers\" is the number of transformer blocks within the model;\n",
    "- \"drop_rate\" is the dropout mechanism's intensity;\n",
    "- \"qkv_bias\" decides if the Linear layers in the multi-head attention mechanism should include a bias vector when computing query (Q), key (K), and value (V) tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a67a6c61-3679-4364-821a-a73c02fafa8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.258417Z",
     "iopub.status.busy": "2025-05-01T09:28:09.258417Z",
     "iopub.status.idle": "2025-05-01T09:28:09.267299Z",
     "shell.execute_reply": "2025-05-01T09:28:09.265772Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.258417Z"
    }
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e49c8de-bf80-433f-aa38-bdfa94418ffa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.270938Z",
     "iopub.status.busy": "2025-05-01T09:28:09.269706Z",
     "iopub.status.idle": "2025-05-01T09:28:09.287150Z",
     "shell.execute_reply": "2025-05-01T09:28:09.287150Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.270938Z"
    }
   },
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Using placeholders for transformer blocks create a dummy GPT\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device = in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c0d63f7-3b1f-4857-b772-9e105254996a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.293298Z",
     "iopub.status.busy": "2025-05-01T09:28:09.291073Z",
     "iopub.status.idle": "2025-05-01T09:28:09.306241Z",
     "shell.execute_reply": "2025-05-01T09:28:09.306241Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.293298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([6109, 3626, 6100,  345]), tensor([6109, 1110, 6622,  257])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple data pipeline for GPT model\n",
    "batch = []\n",
    "\n",
    "txt1 = 'Every effort moves you'\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.extend([torch.tensor(tokenizer.encode(i)) for i in [txt1, txt2]])\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c894f4a-f679-4824-9f6d-a67974eb360a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.310836Z",
     "iopub.status.busy": "2025-05-01T09:28:09.310836Z",
     "iopub.status.idle": "2025-05-01T09:28:09.329415Z",
     "shell.execute_reply": "2025-05-01T09:28:09.327325Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.310836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack(batch, dim = 0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "351afc6f-e52c-40f7-8edb-1a9d85e7b59a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:28:09.331901Z",
     "iopub.status.busy": "2025-05-01T09:28:09.331901Z",
     "iopub.status.idle": "2025-05-01T09:28:10.518592Z",
     "shell.execute_reply": "2025-05-01T09:28:10.518592Z",
     "shell.execute_reply.started": "2025-05-01T09:28:09.331901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b729f88-5ac5-4d96-b199-279c0cf43014",
   "metadata": {},
   "source": [
    "## LayerNormalization\n",
    "\n",
    "- Layer normalization, also known as LayerNorm (Ba et al. 2016), centers the activations of a neural network layer around a mean of 0 and normalizes their variance to 1\n",
    "- This stabilizes training and enables faster convergence to effective weights\n",
    "- Layer normalization is applied both before and after the multi-head attention module within the transformer block; it's also applied before the final output layer\n",
    "\n",
    "![image](https://camo.githubusercontent.com/1bb0018e68d16529b969ccc6b0bca371ed5a7b9a514d1bd3f345c4941c0f463d/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f30352e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e3cacb9-0ed7-405b-ba01-f6a0102eca14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T10:10:49.288095Z",
     "iopub.status.busy": "2025-05-01T10:10:49.288095Z",
     "iopub.status.idle": "2025-05-01T10:10:49.294451Z",
     "shell.execute_reply": "2025-05-01T10:10:49.294451Z",
     "shell.execute_reply.started": "2025-05-01T10:10:49.288095Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661383e-0dff-4e55-a399-8e6a19a26d2e",
   "metadata": {},
   "source": [
    "**Scale and shift**\n",
    "\n",
    "- Note that in addition to performing the normalization by subtracting the mean and dividing by the variance, we added two trainable parameters, a scale and a shift parameter\n",
    "- The initial scale (multiplying by 1) and shift (adding 0) values don't have any effect; however, scale and shift are trainable parameters that the LLM automatically adjusts during training if it is determined that doing so would improve the model's performance on its training task\n",
    "- This allows the model to learn appropriate scaling and shifting that best suit the data it is processing\n",
    "- Note that we also add a smaller value (eps) before computing the square root of the variance; this is to avoid division-by-zero errors if the variance is 0\n",
    "\n",
    "**Biased variance**\n",
    "\n",
    "- In the variance calculation above, setting `unbiased=False` means using the formula $\\frac{\\sum{(x_i-\\hat{x_i})^2}}{n}$ to compute the variance where `n` is the sample size (here, the number of features or columns); this formula does not include Bessel's correction (which uses `n-1` in the denominator), thus providing a biased estimate of the variance\n",
    "\n",
    "- For LLMs, where the embedding dimension `n` is very large, the difference between using `n` and `n-1` is negligible\n",
    "\n",
    "- However, GPT-2 was trained with a biased variance in the normalization layers, which is why we also adopted this setting for compatibility reasons with the pretrained weights that we will load in later chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d9f44-5e97-4296-920e-a1637c918f72",
   "metadata": {},
   "source": [
    "## GELU activations\n",
    "\n",
    "- In deep learning, ReLU (Rectified Linear Unit) activation functions are commonly used due to their simplicity and effectiveness in various neural network architectures\n",
    "- In LLMs, various other types of activation functions are used beyond the traditional ReLU; two notable examples are **GELU** (*Gaussian Error Linear Unit*) and **SwiGLU** (*Swish-Gated Linear Unit*)\n",
    "- **GELU** and **SwiGLU** are more complex, smooth activation functions incorporating Gaussian and sigmoid-gated linear units, respectively, offering better performance for deep learning models, unlike the simpler, piecewise linear function of ReLU\n",
    "\n",
    "**GELU** or *Gaussian Error Linear Unit*. The GELU nonlinearity weights inputs by their percentile, rather than gates inputs by their sign as in ReLU.\n",
    "$$\\text{GELU}\\left(x\\right) = x{P}\\left(X\\leq{x}\\right) = x\\Phi\\left(x\\right) = x \\cdot \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]$$\n",
    "where  $X \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "**GELU** approximation can be written the following way:\n",
    "$$\\text{GELU}(x) \\approx 0.5x\\left(1+\\tanh\\left[\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715x^{3}\\right)\\right]\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbcf792d-454e-491d-a2c5-1425af7a8bbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:45:37.776698Z",
     "iopub.status.busy": "2025-05-01T09:45:37.776698Z",
     "iopub.status.idle": "2025-05-01T09:45:37.789061Z",
     "shell.execute_reply": "2025-05-01T09:45:37.789061Z",
     "shell.execute_reply.started": "2025-05-01T09:45:37.776698Z"
    }
   },
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96f131a5-04cf-4b25-a510-a2b32efe6c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T09:45:38.351938Z",
     "iopub.status.busy": "2025-05-01T09:45:38.349931Z",
     "iopub.status.idle": "2025-05-01T09:45:38.360337Z",
     "shell.execute_reply": "2025-05-01T09:45:38.359323Z",
     "shell.execute_reply.started": "2025-05-01T09:45:38.351938Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a439fbe-1ed7-4263-991b-470e60f7523c",
   "metadata": {},
   "source": [
    "## Shortcut connections\n",
    "\n",
    "*Shortcut* (*skip-*, *residual-*) connections that were firstly introduced in field of computer vision to solve the problem of vanishing gradients are used as shorter paths for pushing inputs to the outputs of each layer. They play a crucial role for gradients stabilization.\n",
    "\n",
    "- A shortcut connection creates an alternative shorter path for the gradient to flow through the network\n",
    "- This is achieved by adding the output of one layer to the output of a later layer, usually skipping one or more layers in between\n",
    "\n",
    "![image](https://camo.githubusercontent.com/b1cb95fee4a11c35cb6ce14a2399ac09875c45dc92d344713a80913e05e04c20/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f31322e776562703f313233)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00834a3a-4680-442b-b9d7-036ca3b886cd",
   "metadata": {},
   "source": [
    "## Transformer block\n",
    "\n",
    "Transformer block forms a solid structure that is the heart of the transformer that contains of the following:\n",
    "- Skip-connections;\n",
    "- Layer Norms;\n",
    "- Multihead attention block;\n",
    "- Feed-Forward netwok with a GELU activation;\n",
    "- Dropout block\n",
    "\n",
    "![image](https://camo.githubusercontent.com/6c8c392f72d5b9e86c94aeb9470beab435b888d24135926f1746eb88e0cc18fb/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f31332e776562703f31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c1b3b38-03e7-4868-827a-338f5b348f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T10:10:55.488685Z",
     "iopub.status.busy": "2025-05-01T10:10:55.488685Z",
     "iopub.status.idle": "2025-05-01T10:10:55.498631Z",
     "shell.execute_reply": "2025-05-01T10:10:55.498631Z",
     "shell.execute_reply.started": "2025-05-01T10:10:55.488685Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.dropout_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dea7926b-7444-48bd-85ed-810c6a2ef908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T10:10:56.337075Z",
     "iopub.status.busy": "2025-05-01T10:10:56.337075Z",
     "iopub.status.idle": "2025-05-01T10:10:56.408095Z",
     "shell.execute_reply": "2025-05-01T10:10:56.408095Z",
     "shell.execute_reply.started": "2025-05-01T10:10:56.337075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb1ffc-cc15-4faa-afeb-f9e51630df76",
   "metadata": {},
   "source": [
    "## GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "384a7049-f8cd-4e15-a7c9-7547bfc9948e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T18:05:29.343641Z",
     "iopub.status.busy": "2025-05-01T18:05:29.343641Z",
     "iopub.status.idle": "2025-05-01T18:05:29.361737Z",
     "shell.execute_reply": "2025-05-01T18:05:29.359730Z",
     "shell.execute_reply.started": "2025-05-01T18:05:29.343641Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "08451a34-55df-4cc4-88fd-4fb9b8335e44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T18:32:07.611463Z",
     "iopub.status.busy": "2025-05-01T18:32:07.609455Z",
     "iopub.status.idle": "2025-05-01T18:32:08.392801Z",
     "shell.execute_reply": "2025-05-01T18:32:08.392135Z",
     "shell.execute_reply.started": "2025-05-01T18:32:07.609455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 1.2428, -0.0599, -0.3957,  ..., -0.2280, -1.0819,  0.7927],\n",
      "         [ 0.8036, -0.1291,  0.5082,  ..., -0.7195,  0.1451, -0.4633],\n",
      "         [ 0.4693, -0.8252,  0.4037,  ..., -0.2769, -0.4315, -0.6399],\n",
      "         [ 1.2097, -0.3363,  0.4473,  ...,  0.4035,  0.0572,  0.5602]],\n",
      "\n",
      "        [[ 1.2733, -0.1520, -1.0307,  ...,  0.1332, -0.8642,  0.5514],\n",
      "         [ 0.3403, -0.5964,  0.6355,  ...,  0.3735,  0.4378, -1.1490],\n",
      "         [-0.0091, -0.4894, -0.0107,  ..., -0.1857, -0.7640, -0.4280],\n",
      "         [ 0.3347, -0.8868, -0.9892,  ..., -0.5269,  0.1438, -0.2993]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e6ac92c-33c4-4a18-b5d9-9dd7204b2f79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T18:10:57.001532Z",
     "iopub.status.busy": "2025-05-01T18:10:57.001532Z",
     "iopub.status.idle": "2025-05-01T18:10:57.012893Z",
     "shell.execute_reply": "2025-05-01T18:10:57.012893Z",
     "shell.execute_reply.started": "2025-05-01T18:10:57.001532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6a1c4-21b5-43f8-876b-9df8eb9a699b",
   "metadata": {},
   "source": [
    "- As we see above, this model has 163M, not 124M parameters; why?\n",
    "- In the original GPT-2 paper, the researchers applied weight tying, which means that they reused the token embedding layer (tok_emb) as the output layer, which means setting `self.out_head.weight` = `self.tok_emb.weight`\n",
    "- The token embedding layer projects the 50,257-dimensional one-hot encoded input tokens to a 768-dimensional embedding representation\n",
    "- The output layer projects 768-dimensional embeddings back into a 50,257-dimensional representation so that we can convert these back into words (more about that in the next section)\n",
    "- So, the embedding and output layer have the same number of weight parameters, as we can see based on the shape of their weight matrices\n",
    "- However, a quick note about its size: we previously referred to it as a 124M parameter model\n",
    "- In the original GPT-2 paper, the researchers reused the token embedding matrix as an output matrix\n",
    "- Correspondingly, if we subtracted the number of parameters of the output layer, we'd get a 124M parameter mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ecac5a9-7dde-4ccd-a10e-c6100877ce68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T18:12:35.515814Z",
     "iopub.status.busy": "2025-05-01T18:12:35.513816Z",
     "iopub.status.idle": "2025-05-01T18:12:35.523333Z",
     "shell.execute_reply": "2025-05-01T18:12:35.522324Z",
     "shell.execute_reply.started": "2025-05-01T18:12:35.514814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39099ebc-ae7d-4d23-b835-480959c029d3",
   "metadata": {},
   "source": [
    "- The memory requirements of the model can be computed as follows, which can be a helpful reference point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab0c7968-f373-41eb-bef2-4d8c26fbe6a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T18:13:23.446454Z",
     "iopub.status.busy": "2025-05-01T18:13:23.446454Z",
     "iopub.status.idle": "2025-05-01T18:13:23.450750Z",
     "shell.execute_reply": "2025-05-01T18:13:23.450750Z",
     "shell.execute_reply.started": "2025-05-01T18:13:23.446454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22983981-7665-4b0f-a034-20e235107294",
   "metadata": {},
   "source": [
    "# Generating text\n",
    "\n",
    "LLM's are autoregressive and can be used to generate text the following way\n",
    "\n",
    "![image](https://camo.githubusercontent.com/be7b35733665766c48c64f651586173df9d1dd3a9ca985eca3593df5355db6a1/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f31362e77656270)\n",
    "\n",
    "- The following `generate_text_simple` function implements **greedy decoding**, which is a simple and fast method to generate text\n",
    "- In greedy decoding, at each step, the model chooses the word (or token) with the highest probability as its next output (the highest logit corresponds to the highest probability, so we technically wouldn't even have to compute the softmax function explicitly)\n",
    "- However there exist a big variety of text generating strategies that implement quite sophisticated methods of text generation like: **top-k sampling**, **beam search** and others.\n",
    "\n",
    "**Typical text generation**\n",
    "\n",
    "The process of text generation contains of multiple steps that form an autoregressive nature of a generative model:\n",
    "1. Output tensors decoding;\n",
    "2. Token selection (based on probability or other strategies);\n",
    "3. Token conversion to human-like text.\n",
    "\n",
    "![img](https://camo.githubusercontent.com/1e55260d5caca3ac6bd1215ca36973e0a0779995487ed9486f510e5baa108072/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f31372e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5921715a-23da-4c90-a362-78ba04a9f818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:23:30.008258Z",
     "iopub.status.busy": "2025-05-02T08:23:30.008258Z",
     "iopub.status.idle": "2025-05-02T08:23:30.014487Z",
     "shell.execute_reply": "2025-05-02T08:23:30.014487Z",
     "shell.execute_reply.started": "2025-05-02T08:23:30.008258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Greedy decoding\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size): # idx - [batch, n_tokens]\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:] # crops current context in order to comply with the available context window\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]  # chose last vector of vocab_size corresponding to the next token probability distribution\n",
    "        probas = torch.softmax(logits, dim = -1)\n",
    "        idx_next = torch.argmax(probas, dim = -1, keepdim = True)\n",
    "        idx = torch.cat((idx, idx_next), dim = 1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1472f2b-54b2-4a06-8677-7a55f954d5e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T18:32:38.202341Z",
     "iopub.status.busy": "2025-05-01T18:32:38.202341Z",
     "iopub.status.idle": "2025-05-01T18:32:38.212669Z",
     "shell.execute_reply": "2025-05-01T18:32:38.212669Z",
     "shell.execute_reply.started": "2025-05-01T18:32:38.202341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716, 257, 3644]\n",
      "encoded_tensor.shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am a computer\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "193adbd2-8a87-425d-96d6-ae855389e8dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T18:32:39.504820Z",
     "iopub.status.busy": "2025-05-01T18:32:39.504820Z",
     "iopub.status.idle": "2025-05-01T18:32:39.621981Z",
     "shell.execute_reply": "2025-05-01T18:32:39.621587Z",
     "shell.execute_reply.started": "2025-05-01T18:32:39.504820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716,   257,  3644, 15496,    11,   314,   716,\n",
      "           257,  3644, 15496,    11,   314,   716,   257,  3644, 15496,    11,\n",
      "           314,   716,   257,  3644]])\n",
      "Output length: 24\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=2, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "260487ac-c08b-412b-93bf-bbcc44f740bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T18:32:40.296636Z",
     "iopub.status.busy": "2025-05-01T18:32:40.294629Z",
     "iopub.status.idle": "2025-05-01T18:32:40.303966Z",
     "shell.execute_reply": "2025-05-01T18:32:40.302521Z",
     "shell.execute_reply.started": "2025-05-01T18:32:40.296636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am a computerHello, I am a computerHello, I am a computerHello, I am a computer\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
