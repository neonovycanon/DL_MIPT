{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86036716-d58b-482d-b144-125ede017b4e",
   "metadata": {},
   "source": [
    "# Training generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd870340-9bda-4d3b-9d0e-8daad8b34a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:43.337865Z",
     "iopub.status.busy": "2025-05-02T09:52:43.337865Z",
     "iopub.status.idle": "2025-05-02T09:52:45.383297Z",
     "shell.execute_reply": "2025-05-02T09:52:45.383297Z",
     "shell.execute_reply.started": "2025-05-02T09:52:43.337865Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import urllib.request\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformer import GPTModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b3e85-3203-4486-bd05-765aa3a301a6",
   "metadata": {},
   "source": [
    "# Evaluating generative text models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e155707b-ca8b-407e-92ed-d44f6c7a79b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:45.384304Z",
     "iopub.status.busy": "2025-05-02T09:52:45.384304Z",
     "iopub.status.idle": "2025-05-02T09:52:45.387919Z",
     "shell.execute_reply": "2025-05-02T09:52:45.387919Z",
     "shell.execute_reply.started": "2025-05-02T09:52:45.384304Z"
    }
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117df5af-8ee3-4750-8b36-01fa51965411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:45.387919Z",
     "iopub.status.busy": "2025-05-02T09:52:45.387919Z",
     "iopub.status.idle": "2025-05-02T09:52:45.913888Z",
     "shell.execute_reply": "2025-05-02T09:52:45.913888Z",
     "shell.execute_reply.started": "2025-05-02T09:52:45.387919Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d21e40-f58c-4ce7-bae9-b6d7c72aea0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:45.913888Z",
     "iopub.status.busy": "2025-05-02T09:52:45.913888Z",
     "iopub.status.idle": "2025-05-02T09:52:45.919212Z",
     "shell.execute_reply": "2025-05-02T09:52:45.919212Z",
     "shell.execute_reply.started": "2025-05-02T09:52:45.913888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Greedy decoding\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size): # idx - [batch, n_tokens]\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:] # crops current context in order to comply with the available context window\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]  # chose last vector of vocab_size corresponding to the next token probability distribution\n",
    "        probas = torch.softmax(logits, dim = -1)\n",
    "        idx_next = torch.argmax(probas, dim = -1, keepdim = True)\n",
    "        idx = torch.cat((idx, idx_next), dim = 1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Create batch dimension in order to comply with models\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2947d724-20e3-4a8c-b563-bccdb0076347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:45.920022Z",
     "iopub.status.busy": "2025-05-02T09:52:45.920022Z",
     "iopub.status.idle": "2025-05-02T09:52:46.358533Z",
     "shell.execute_reply": "2025-05-02T09:52:46.358533Z",
     "shell.execute_reply.started": "2025-05-02T09:52:45.920022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3e6e66-8fa1-4d47-a484-8f9775a519e3",
   "metadata": {},
   "source": [
    "## Calculating the text generation loss: cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9f750f-f832-44b3-9ace-c88a9a699536",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.358533Z",
     "iopub.status.busy": "2025-05-02T09:52:46.358533Z",
     "iopub.status.idle": "2025-05-02T09:52:46.362615Z",
     "shell.execute_reply": "2025-05-02T09:52:46.362615Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.358533Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d8ec10-d10e-4a76-9b74-4c7db03dc92d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.363203Z",
     "iopub.status.busy": "2025-05-02T09:52:46.363203Z",
     "iopub.status.idle": "2025-05-02T09:52:46.397544Z",
     "shell.execute_reply": "2025-05-02T09:52:46.397544Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.363203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701f36f-632b-438e-8ec2-e75db57d4856",
   "metadata": {},
   "source": [
    "Calculations performed by model expose probability distribution over all token vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b69968b2-d5e6-487d-92d3-503c670ee91d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.398552Z",
     "iopub.status.busy": "2025-05-02T09:52:46.397544Z",
     "iopub.status.idle": "2025-05-02T09:52:46.403418Z",
     "shell.execute_reply": "2025-05-02T09:52:46.403418Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.398552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a99daf-e465-4a1c-8730-3c55111be2b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.403418Z",
     "iopub.status.busy": "2025-05-02T09:52:46.403418Z",
     "iopub.status.idle": "2025-05-02T09:52:46.408724Z",
     "shell.execute_reply": "2025-05-02T09:52:46.408724Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.403418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "# The token probabilities corresponding to the target indices are as follows:\n",
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbb70727-906b-4732-86f5-22fc5c1cb9ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.408724Z",
     "iopub.status.busy": "2025-05-02T09:52:46.408724Z",
     "iopub.status.idle": "2025-05-02T09:52:46.414838Z",
     "shell.execute_reply": "2025-05-02T09:52:46.414838Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.408724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n",
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)\n",
    "\n",
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852bb746-dd5c-41db-ae64-cdf172064661",
   "metadata": {},
   "source": [
    "- The goal is to make this average log probability as large as possible by optimizing the model weights\n",
    "- Due to the `log`, the largest possible value is 0, and we are currently far away from 0\n",
    "- In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the negative average log-probability value; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0\n",
    "- The value negative of -10.7722, i.e., 10.7722, is also called **cross-entropy loss** in deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e2393f-0a2b-4dfe-be74-7533430c267c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.414838Z",
     "iopub.status.busy": "2025-05-02T09:52:46.414838Z",
     "iopub.status.idle": "2025-05-02T09:52:46.418809Z",
     "shell.execute_reply": "2025-05-02T09:52:46.418809Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.414838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69882c8-4414-44b7-b875-350ef0e2d6da",
   "metadata": {},
   "source": [
    "- We can use PyTorch's `cross-entropy` loss for this simple task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d1aced3-e82c-4e1b-8aae-618d6d340987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.418809Z",
     "iopub.status.busy": "2025-05-02T09:52:46.418809Z",
     "iopub.status.idle": "2025-05-02T09:52:46.422254Z",
     "shell.execute_reply": "2025-05-02T09:52:46.422254Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.418809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f220475f-0c0c-4f81-ae66-73f291fc2411",
   "metadata": {},
   "source": [
    "For the `cross_entropy` function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e00940ea-753b-4623-925a-9a8ca8ac866f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.422254Z",
     "iopub.status.busy": "2025-05-02T09:52:46.422254Z",
     "iopub.status.idle": "2025-05-02T09:52:46.426677Z",
     "shell.execute_reply": "2025-05-02T09:52:46.426677Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.422254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9db3e-34e2-4327-a152-cc436382bd10",
   "metadata": {},
   "source": [
    "- Targets are the token IDs, which also represent the index positions in the logits tensors that we want to maximize\n",
    "- The `cross_entropy` function in PyTorch will automatically take care of applying the softmax and log-probability computation internally over those token indices in the logits that are to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cb7e6ac-66c4-462c-b16f-c2957cf259f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.427916Z",
     "iopub.status.busy": "2025-05-02T09:52:46.427916Z",
     "iopub.status.idle": "2025-05-02T09:52:46.432560Z",
     "shell.execute_reply": "2025-05-02T09:52:46.432560Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.427916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3021975e-35d8-4b8c-bce9-965f7a5e6df9",
   "metadata": {},
   "source": [
    "- A concept related to the `cross-entropy loss` is the `perplexity` of an LLM\n",
    "- The `perplexity` is simply the exponential of the cross-entropy loss\n",
    "$$PP(w) = \\sqrt[k]{\\frac{1}{\\prod_{i=0}^{k} P(w_i|w_{i-1},...,w_0)}} = \\exp{\\biggr(-\\frac{1}{k}\\sum_{i=0}^{k} \\log [P_{\\theta}(w_i|w_{i-1},...,w_0)] \\biggl)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb369e8c-4262-4561-bb7c-117deba0dd76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.432560Z",
     "iopub.status.busy": "2025-05-02T09:52:46.432560Z",
     "iopub.status.idle": "2025-05-02T09:52:46.436235Z",
     "shell.execute_reply": "2025-05-02T09:52:46.436235Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.432560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94f910-0408-40f8-a125-867a85ebeae6",
   "metadata": {},
   "source": [
    "- The `perplexity` is often considered more interpretable because it can be understood as the *effective vocabulary size that the model is uncertain about at each step* (in the example above, that'd be 52,918 words or tokens)\n",
    "- Perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset\n",
    "- Similar to the loss, a *lower perplexity* indicates that the *model predictions are closer to the actual distribution*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de45bb8-7b2c-4b3d-8da8-227f28e21d2d",
   "metadata": {},
   "source": [
    "## Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ebdfe3-2f74-420e-b839-a3cb86e8cbe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.436235Z",
     "iopub.status.busy": "2025-05-02T09:52:46.436235Z",
     "iopub.status.idle": "2025-05-02T09:52:46.440346Z",
     "shell.execute_reply": "2025-05-02T09:52:46.440346Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.436235Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join(\"data\", \"the-verdict.txt\")\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0e8e01a-1563-47a1-bad2-09b6ec022b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.440346Z",
     "iopub.status.busy": "2025-05-02T09:52:46.440346Z",
     "iopub.status.idle": "2025-05-02T09:52:46.444570Z",
     "shell.execute_reply": "2025-05-02T09:52:46.444033Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.440346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "710f6ba2-ce97-4af1-a42b-625f5d76d986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.444570Z",
     "iopub.status.busy": "2025-05-02T09:52:46.444570Z",
     "iopub.status.idle": "2025-05-02T09:52:46.451132Z",
     "shell.execute_reply": "2025-05-02T09:52:46.451132Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.444570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7834d82b-ee8e-40a9-b510-cb3b9c7ae04e",
   "metadata": {},
   "source": [
    "- We divide the dataset into a training and a validation set and use the data loaders from chapter 2 to prepare the batches for LLM training\n",
    "- For visualization purposes, the figure below assumes a `max_length=6`, but for the training loader, we set the max_length equal to the context length that the LLM supports\n",
    "- The figure below only shows the input tokens for simplicity\n",
    "    - Since we train the LLM to predict the next word in the text, the targets look the same as these inputs, except that the targets are shifted by one position\n",
    "\n",
    "![img](https://camo.githubusercontent.com/4d6c167e344077ad4c3b94cbbd6a9059589021bec84fa280dd552bfcddeb4918/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830355f636f6d707265737365642f6261746368696e672e77656270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "757f0691-ee0d-43ae-ba02-278c4530e2e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.451132Z",
     "iopub.status.busy": "2025-05-02T09:52:46.451132Z",
     "iopub.status.idle": "2025-05-02T09:52:46.460260Z",
     "shell.execute_reply": "2025-05-02T09:52:46.460260Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.451132Z"
    }
   },
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77882198-d0d6-43f3-871b-49f0a47b1089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.461254Z",
     "iopub.status.busy": "2025-05-02T09:52:46.460260Z",
     "iopub.status.idle": "2025-05-02T09:52:46.463443Z",
     "shell.execute_reply": "2025-05-02T09:52:46.463443Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.461254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dad91a75-7d1f-4114-bc33-ed09eb971179",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.463443Z",
     "iopub.status.busy": "2025-05-02T09:52:46.463443Z",
     "iopub.status.idle": "2025-05-02T09:52:46.469664Z",
     "shell.execute_reply": "2025-05-02T09:52:46.469664Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.463443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d421412-a45b-4d0a-bb95-cdd7245bf00b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.469664Z",
     "iopub.status.busy": "2025-05-02T09:52:46.469664Z",
     "iopub.status.idle": "2025-05-02T09:52:46.474380Z",
     "shell.execute_reply": "2025-05-02T09:52:46.474380Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.469664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e79ab77-f455-46b2-b1a5-f25e403de9da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.474380Z",
     "iopub.status.busy": "2025-05-02T09:52:46.474380Z",
     "iopub.status.idle": "2025-05-02T09:52:46.478998Z",
     "shell.execute_reply": "2025-05-02T09:52:46.478998Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.474380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss calculation per batch\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "467ecb33-3067-4fe2-9601-90e12d8ac5d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:46.480006Z",
     "iopub.status.busy": "2025-05-02T09:52:46.480006Z",
     "iopub.status.idle": "2025-05-02T09:52:47.089561Z",
     "shell.execute_reply": "2025-05-02T09:52:47.089561Z",
     "shell.execute_reply.started": "2025-05-02T09:52:46.480006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583584255642\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351fe83e-8201-4590-b7af-47f97e4bb6c2",
   "metadata": {},
   "source": [
    "# Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf4798bf-7844-4f6e-8178-aa6a309c0f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:47.089561Z",
     "iopub.status.busy": "2025-05-02T09:52:47.089561Z",
     "iopub.status.idle": "2025-05-02T09:52:47.098844Z",
     "shell.execute_reply": "2025-05-02T09:52:47.098443Z",
     "shell.execute_reply.started": "2025-05-02T09:52:47.089561Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model, encoded, 50, context_size)\n",
    "\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                      optimizer, device, num_epochs,\n",
    "                      eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70534efb-5001-48d9-9cb1-81690337d13f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:52:47.100243Z",
     "iopub.status.busy": "2025-05-02T09:52:47.099865Z",
     "iopub.status.idle": "2025-05-02T09:53:08.307263Z",
     "shell.execute_reply": "2025-05-02T09:53:08.307263Z",
     "shell.execute_reply.started": "2025-05-02T09:52:47.100243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.813, Val loss 9.921\n",
      "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.334\n",
      "very effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.622, Val loss 7.046                   \n",
      "\n",
      "5): Train loss 6.048, Val loss 6.600                   \n",
      "\n",
      "\n",
      "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
      "p 3 (Step 000020): Train loss 5.580, Val loss 6.484                             \n",
      "\n",
      "Ep 3 (Step 000025): Train loss 5.504, Val loss 6.402                            \n",
      "\n",
      "\n",
      "very effort moves you, and, and of the of the of the, and I had. G. Gis, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,\n",
      "Ep 4 (Step 000030): Train loss 5.079, Val loss 6.319, and I had. G. Gis, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,\n",
      "\n",
      "5): Train loss 4.826, Val loss 6.311, and I had. G. Gis, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,\n",
      "\n",
      "\n",
      "Every effort moves you, and I had a a of the picture to the picture. Gisburn, and I had been--and of the of the of the of the of the of the of the of the of the of the of the of the of the of\n",
      "p 5 (Step 000040): Train loss 4.263, Val loss 6.200 to the picture. Gisburn, and I had been--and of the of the of the of the of the of the of the of the of the of the of the of the of the of\n",
      "\n",
      "Every effort moves you, I felt, and I felt--I had the fact of the last I had been the picture, I felt, I had been the of the picture--as Jack himself, as a of the of the donkey, and I felt he had been\n",
      "p 6 (Step 000045): Train loss 3.856, Val loss 6.133e fact of the last I had been the picture, I felt, I had been the of the picture--as Jack himself, as a of the of the donkey, and I felt he had been\n",
      "\n",
      "Ep 6 (Step 000050): Train loss 3.309, Val loss 6.144 fact of the last I had been the picture, I felt, I had been the of the picture--as Jack himself, as a of the of the donkey, and I felt he had been\n",
      "\n",
      "\n",
      "very effort moves you know the \"Oh, and he was not the fact.               \"Oh, and I had a little.             the of the picture--as Jack himself, as a of the of the donkey, and I felt he had been\n",
      "\n",
      "\n",
      "\n",
      "Ep 7 (Step 000055): Train loss 3.279, Val loss 6.188he fact.               \"Oh, and I had a little.             the of the picture--as Jack himself, as a of the of the donkey, and I felt he had been\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "60): Train loss 2.534, Val loss 6.123he fact.               \"Oh, and I had a little.             the of the picture--as Jack himself, as a of the of the donkey, and I felt he had been\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Every effort moves you know the picture, and pushed one of the deep arm-chairs forward. \"There: \"Yes, and in fact, and that, and I was his pictures--and it.   \"I looked up his pictures--and by his\n",
      "p 8 (Step 000065): Train loss 2.072, Val loss 6.148one of the deep arm-chairs forward. \"There: \"Yes, and in fact, and that, and I was his pictures--and it.   \"I looked up his pictures--and by his\n",
      "\n",
      "Ep 8 (Step 000070): Train loss 1.735, Val loss 6.191ne of the deep arm-chairs forward. \"There: \"Yes, and in fact, and that, and I was his pictures--and it.   \"I looked up his pictures--and by his\n",
      "\n",
      "\n",
      "very effort moves you know,\" was not that the picture for nothing--I had a good-t. Gisburn's an!     \"Oh, and I had been the donkey. It was, the donkey. \"There were days when Ictures--and by his\n",
      "\n",
      "\n",
      "\n",
      "Ep 9 (Step 000075): Train loss 1.367, Val loss 6.228e for nothing--I had a good-t. Gisburn's an!     \"Oh, and I had been the donkey. It was, the donkey. \"There were days when Ictures--and by his\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "80): Train loss 1.051, Val loss 6.241e for nothing--I had a good-t. Gisburn's an!     \"Oh, and I had been the donkey. It was, the donkey. \"There were days when Ictures--and by his\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Every effort moves you know,\" was not that my hostess was \"interesting\": on that I felt to have given Miss Croft the fact, and that, in the moment--as Jack himself, his pictures--the quality of Jack's \"strongest,\" she was\n",
      "p 10 (Step 000085): Train loss 0.794, Val loss 6.350 was \"interesting\": on that I felt to have given Miss Croft the fact, and that, in the moment--as Jack himself, his pictures--the quality of Jack's \"strongest,\" she was\n",
      "\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when Ist,\" she was\n",
      "\n",
      "\n",
      "Training completed in 0.35 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95b3c960-543c-4d5f-a928-339b56773f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:53:48.654468Z",
     "iopub.status.busy": "2025-05-02T09:53:48.654468Z",
     "iopub.status.idle": "2025-05-02T09:53:49.607471Z",
     "shell.execute_reply": "2025-05-02T09:53:49.607471Z",
     "shell.execute_reply.started": "2025-05-02T09:53:48.654468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streets are fantastic sunburnt cheeks furrowed by a smile that lifted the tips of a self-confident moustache, I had made him--it was fitting that they should mourn him. It had longed to say: \"Be dissatisfied with your\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_sample(model, tokenizer, 'cuda', 'Streets are fantastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88735efe-4621-4493-a764-99e4d9461e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
