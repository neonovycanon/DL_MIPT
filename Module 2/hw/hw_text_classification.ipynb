{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0ADTojbpfLt"
   },
   "source": [
    "# Домашнее задание. Нейросетевая классификация текстов\n",
    "\n",
    "В этом домашнем задании вам предстоит самостоятельно решить задачу классификации текстов на основе семинарского кода. Мы будем использовать датасет [ag_news](https://paperswithcode.com/dataset/ag-news). Это датасет для классификации новостей на 4 темы: \"World\", \"Sports\", \"Business\", \"Sci/Tech\".\n",
    "\n",
    "Установим модуль datasets, чтобы нам проще было работать с данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18487,
     "status": "ok",
     "timestamp": 1679310682713,
     "user": {
      "displayName": "Deep Learning School",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "p2QW_jCR0_kh",
    "outputId": "3fc85cb6-65a4-4bd8-f565-820c6b7504b4"
   },
   "outputs": [],
   "source": [
    "!pip install datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rh9ZXSeCpng9"
   },
   "source": [
    "Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOJi16bLpd_O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import string\n",
    "\n",
    "import seaborn\n",
    "seaborn.set(palette='summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1679310692063,
     "user": {
      "displayName": "Deep Learning School",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "91JuM0SQvXud",
    "outputId": "1a67bb6e-1edd-4ded-f018-ed9aa431c7db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1679310695816,
     "user": {
      "displayName": "Deep Learning School",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "adJC8ShFq9HM",
    "outputId": "c85c95cb-f61e-4d37-ca8a-029c57d67e6f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwsfS1ENq5ig"
   },
   "source": [
    "## Подготовка данных\n",
    "Для вашего удобства, мы привели код обработки датасета в ноутбуке. Ваша задача --- обучить модель, которая получит максимальное возможное качество на тестовой части. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244,
     "referenced_widgets": [
      "1eb740874a8b4df68e18f0dbf8e7acdc",
      "fc9cdace4f17455082a2a8374913d322",
      "3ebe7107b9c643a4a990b0c5966299e8",
      "f1bde9957cab40ba97cd7faa65889b00",
      "aefc7fff46e6439fae50d2ac508d78e3",
      "33ce5077e4714f239f12562e1a7a7ef2",
      "f255118df0ea49fa8feb56d043cf87b6",
      "4acffbd4a4604620bcd4b4f8c9c6af9b",
      "8cde1d20ba3e45a08490e769ddd51bb4",
      "513f35a7ebb74d3ebbaefa013b725d14",
      "81311bdb923b4eca97431b56195acd58",
      "09ec9cea96b341d0a25451b192102f39",
      "c9ce26882ff34d61b1817915058152f1",
      "ece019411eba48ca93173a9b15cf8cbb",
      "19722e928d404adb990c5cec48524bba",
      "a93760269c044501aa63f3af2524210e",
      "6a76b678cb8e49d99b307e505c5baf43",
      "991528abe6eb42e29b4303bd72ecbb9c",
      "5b39c3103bf74237a2e03e6fdb7da027",
      "0847a33379184a368d5d1b82222b74cc",
      "122ed5c8b5a74305ba54ff99af7e9d54",
      "c0894fc6901f475382628d4868062558",
      "d76f8b4fbb004812b2bdc3f614cdc307",
      "e684e9dab3dc4a04b4fac5cab926f8fe",
      "a3a2891c78654d639a25ff7522357b26",
      "7caa28a01d5541e98ca6d2bd0c3f0a4c",
      "f5a54984249a47dd8ac113ce96ddbb71",
      "58df47e13b1e48a8863e48d6a12a98b5",
      "a6b4481c94ad4af181ff0934ee7fca92",
      "ca007860138646589925567fa12ee1f9",
      "903b7cdb57de41459650c226b5629d05",
      "196e3535fb3b4ede9c86fe2c79009322",
      "e2cbb1d5b4e1497e804bf00332a14743",
      "a2422c15e5ba42bf87eb6fdcd0604ec1",
      "96f4ff5cfc7b40e995a6c8ecd3546f6b",
      "bc6ee58abaa643cd9643064537784d64",
      "df0ff75a6c5a474295564794954addc4",
      "540219b4cc9b4ace9c70feb8a8146480",
      "b3ba7361fe3b46e3a208bf6416131f28",
      "feac1851ed644eeb98fb9ac9aadd7f41",
      "ccf9677bca0141dba7ba27dde42276e1",
      "0401a08736004c4cafe05feae921849f",
      "dc1d3746c5cf4817b8e80bf92ad02d71",
      "33521ca994f14e55aa040cfc2f5d1105",
      "4ceb025f703948e4971486323e58c3e4",
      "8136ea38174b46fe99d4549e396adbaf",
      "596c2a7e529b45a7973496e02b50ee17",
      "04c14b466921444299752b1bf6cac9fb",
      "99b28ecd95564dbc99ef588f42ae1dd2",
      "5494c2f8716a4da0a846bfad0e25b425",
      "f5d44bab5deb4be18b828d9c78feb5da",
      "dcab6933e7e6483fb37ef3ea4f2f96cb",
      "3018b5e2177b4eb7ba0f562b7db28fe4",
      "3bdb6917a86249c6a094d5d8e1433365",
      "a1f665b2995b4d62a2881de651810d44",
      "38c3fbf1198e4f7cbfc990cb362be0ba",
      "9fbc7d78c453476c8a6ace8a267aa6f3",
      "88bf756ae2ef454da07e153e26d6b272",
      "0872109d55664634958251b1c8a3879e",
      "e43cb91bee254131af50f2af50be3a4e",
      "cb8cdb1fcd394e79be9fcfb5708fee31",
      "55b7456ea0be447a8b87f75226163726",
      "6c7372f597dc48b2bd71771c13fbe1ca",
      "e7a723bfbede426a9c9184ed02ff2574",
      "1644c4f507954f0ba03aeb75b80b93ba",
      "bc1cab86f2d6492cb79c21c25206955e",
      "11d52f8a43cf44f08db4d15e0609adfd",
      "dc7219cdf164455e9172bc134d89d35a",
      "58ed943442364505881079688d1fde0d",
      "24e73abed20f4880a8d235907e0df027",
      "056cc6bc26f14d1aa27208e4be38bb73",
      "9e2a2ae4fcb54b5ca9f2d714b10061a6",
      "c4c47a154f2d40deaddc48a393ca4526",
      "a1605863309c42169162d60f6c6475b8",
      "5340dd684eed471ba18cb36d477c9f4d",
      "95572bae4c6f4d85b85202d04b4271a3",
      "a3fb5f712cdf4d3da74af03d24622ada",
      "640d1239e46f4a4d9224259f19408dee",
      "d635465d15d84b4ea2604e46de5fc957",
      "39e541c8faca418782958233ef684677",
      "0c75bfe839864746be85eae85414c224",
      "9e7c10778b7d4d6e89355acfa3a90fa0",
      "50565855965a4fc08310e2b7d8d65fd0",
      "4b8e4f5ccb3346fdb8a8e56baf8a7f49",
      "3d64d3a55ba64a5da8776712d649fc4e",
      "93b7be412ced4982a8d7901c40e8c64d",
      "31e4423718424bc2bbca5ac1a9051b08",
      "2b12a038d1864df79f83d85db191cfbb"
     ]
    },
    "executionInfo": {
     "elapsed": 12911,
     "status": "ok",
     "timestamp": 1679310711937,
     "user": {
      "displayName": "Deep Learning School",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "qHLNWOfJqSfc",
    "outputId": "14e2b6e7-c1e6-48f9-f97a-bff172f80d3e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb740874a8b4df68e18f0dbf8e7acdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ec9cea96b341d0a25451b192102f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76f8b4fbb004812b2bdc3f614cdc307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset ag_news/default to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2422c15e5ba42bf87eb6fdcd0604ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ceb025f703948e4971486323e58c3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/751k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c3fbf1198e4f7cbfc990cb362be0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d52f8a43cf44f08db4d15e0609adfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ag_news downloaded and prepared to /root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640d1239e46f4a4d9224259f19408dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Загрузим датасет\n",
    "dataset = datasets.load_dataset('ag_news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYG67ZJj4hZS"
   },
   "source": [
    "Как и в семинаре, выполним следующие шаги:\n",
    "* Составим словарь\n",
    "* Создадим класс WordDataset\n",
    "* Выделим обучающую и тестовую часть, создадим DataLoader-ы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "45b415cc22e64cbc9376f7bf8601f09b",
      "eee83e0a1bd14d00af3159061f96a553",
      "bd82244ccbd842539464a9168a897f1c",
      "94b4879a5a644dbeaa3ec1e5af2b1744",
      "fb542b520c1a42f4b621f95a4e90d644",
      "87447d7825574c6f8ab503b1b534d506",
      "56d638bddd534750b06ea6fe6701cb15",
      "7358823ad0d1474cae97638aebeb835f",
      "f53391ef64574c76bd18add79c0fa4bf",
      "6280438c65704c3a9603a2402d18c39a",
      "fe99814191c14edea16dc733737441a5"
     ]
    },
    "executionInfo": {
     "elapsed": 25172,
     "status": "ok",
     "timestamp": 1679310737099,
     "user": {
      "displayName": "Deep Learning School",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "nEvCN0Y1w1yH",
    "outputId": "7aaf0b16-1e5a-4b5a-f750-d2a0e93adf04"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b415cc22e64cbc9376f7bf8601f09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 11842\n"
     ]
    }
   ],
   "source": [
    "words = Counter()\n",
    "\n",
    "for example in tqdm(dataset['train']['text']):\n",
    "    # Приводим к нижнему регистру и убираем пунктуацию\n",
    "    prccessed_text = example.lower().translate(\n",
    "        str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    for word in word_tokenize(prccessed_text):\n",
    "        words[word] += 1\n",
    "\n",
    "\n",
    "vocab = set(['<unk>', '<bos>', '<eos>', '<pad>'])\n",
    "counter_threshold = 25\n",
    "\n",
    "for char, cnt in words.items():\n",
    "    if cnt > counter_threshold:\n",
    "        vocab.add(char)\n",
    "\n",
    "print(f'Размер словаря: {len(vocab)}')\n",
    "\n",
    "word2ind = {char: i for i, char in enumerate(vocab)}\n",
    "ind2word = {i: char for char, i in word2ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVzXL17PzC7K"
   },
   "outputs": [],
   "source": [
    "class WordDataset:\n",
    "    def __init__(self, sentences):\n",
    "        self.data = sentences\n",
    "        self.unk_id = word2ind['<unk>']\n",
    "        self.bos_id = word2ind['<bos>']\n",
    "        self.eos_id = word2ind['<eos>']\n",
    "        self.pad_id = word2ind['<pad>']\n",
    "\n",
    "    def __getitem__(self, idx: int) -> List[int]:\n",
    "        processed_text = self.data[idx]['text'].lower().translate(\n",
    "            str.maketrans('', '', string.punctuation))\n",
    "        tokenized_sentence = [self.bos_id]\n",
    "        tokenized_sentence += [\n",
    "            word2ind.get(word, self.unk_id) for word in word_tokenize(processed_text)\n",
    "            ] \n",
    "        tokenized_sentence += [self.eos_id]\n",
    "\n",
    "        train_sample = {\n",
    "            \"text\": tokenized_sentence,\n",
    "            \"label\": self.data[idx]['label']\n",
    "        }\n",
    "\n",
    "        return train_sample\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def collate_fn_with_padding(\n",
    "    input_batch: List[List[int]], pad_id=word2ind['<pad>'], max_len=256) -> torch.Tensor:\n",
    "    seq_lens = [len(x['text']) for x in input_batch]\n",
    "    max_seq_len = min(max(seq_lens), max_len)\n",
    "\n",
    "    new_batch = []\n",
    "    for sequence in input_batch:\n",
    "        sequence['text'] = sequence['text'][:max_seq_len]\n",
    "        for _ in range(max_seq_len - len(sequence['text'])):\n",
    "            sequence['text'].append(pad_id)\n",
    "\n",
    "        new_batch.append(sequence['text'])\n",
    "    \n",
    "    sequences = torch.LongTensor(new_batch).to(device)\n",
    "    labels = torch.LongTensor([x['label'] for x in input_batch]).to(device)\n",
    "\n",
    "    new_batch = {\n",
    "        'input_ids': sequences,\n",
    "        'label': labels\n",
    "    }\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xmeK9Ys1BIG"
   },
   "outputs": [],
   "source": [
    "train_dataset = WordDataset(dataset['train'])\n",
    "\n",
    "np.random.seed(42)\n",
    "idx = np.random.choice(np.arange(len(dataset['test'])), 5000)\n",
    "eval_dataset = WordDataset(dataset['test'].select(idx))\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=collate_fn_with_padding, batch_size=batch_size)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, shuffle=False, collate_fn=collate_fn_with_padding, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7LHfh3u5Bie"
   },
   "source": [
    "## Постановка задачи\n",
    "Ваша задача -- получить максимальное возможное accuracy на `eval_dataloader`. Ниже приведена функция, которую вам необходимо запустить для обученной модели, чтобы вычислить качество её работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDZw4Sehn4NE"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataloader) -> float:\n",
    "    \"\"\"\n",
    "    Calculate accuracy on validation dataloader.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            logits = model(batch['input_ids'])\n",
    "            predictions.append(logits.argmax(dim=1))\n",
    "            target.append(batch['label'])\n",
    "    \n",
    "    predictions = torch.cat(predictions)\n",
    "    target = torch.cat(target)\n",
    "    accuracy = (predictions == target).float().mean().item()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMAexY7Y45E4"
   },
   "source": [
    "## Ход работы\n",
    "Оценка за домашнее задание складывается из четырех частей:\n",
    "### Запуск базовой модели с семинара на новом датасете (1 балл)\n",
    "На семинаре мы создали модель, которая дает на нашей задаче довольно высокое качество. Ваша цель --- обучить ее и вычислить `score`, который затем можно будет использовать в качестве бейзлайна. \n",
    "\n",
    "В модели появится одно важное изменение: количество классов теперь равно не 2, а 4. Обратите на это внимание и найдите, что в коде создания модели нужно модифицировать, чтобы учесть это различие. \n",
    "\n",
    "### Проведение экспериментов по улучшению модели (2 балла за каждый эксперимент)\n",
    "Чтобы улучшить качество базовой модели, можно попробовать различные идеи экспериментов. Каждый выполненный эксперимент будет оцениваться в 2 балла. Для получения полного балла за этот пункт вам необходимо выполнить по крайней мере 2 эксперимента. Не расстраивайтесь, если какой-то эксперимент не дал вам прироста к качеству: он все равно зачтется, если выполнен корректно. \n",
    "\n",
    "Вот несколько идей экспериментов:\n",
    "* **Модель RNN**. Попробуйте другие нейросетевые модели --- LSTM и GRU. Мы советуем обратить внимание на [GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html), так как интерфейс этого класса ничем не отличается от обычной Vanilla RNN, которую мы использовали на семинаре.\n",
    "* **Увеличение количества рекуррентных слоев модели**. Это можно сделать с помощью параметра `num_layers` в классе `nn.RNN`. В такой модели выходы первой RNN передаются в качестве входов второй RNN и так далее. \n",
    "* **Изменение архитектуры после применения RNN**. В базовой модели используется агрегация со всех эмбеддингов. Возможно, вы захотите конкатенировать результат агрегации и эмбеддинг с последнего токена.\n",
    "* **Подбор гиперпараметров и обучение до сходимости**. Возможно, для получения более высокого качества просто необходимо увеличить количество эпох обучения нейросети, а также попробовать различные гиперпараметры: размер словаря, `dropout_rate`, `hidden_dim`.\n",
    "\n",
    "Обратите внимание, что главное правило проведения экспериментов --- необходимо совершать одно архитектурное изменение в одном эксперименте. Если вы совершите несколько изменений, то будет неясно, какое именно из изменений дало прирост к качеству. \n",
    "\n",
    "### Получение высокого качества (3 балла)\n",
    "В конце вашей работы вы должны указать, какая из моделей дала лучший результат, и вывести качество, которое дает лучшая модель, с помощью функции `evaluate`. Ваша модель будет оцениваться по метрике `accuracy` следующим образом:\n",
    "* $accuracy < 0.9$ --- 0 баллов;\n",
    "* $0.9 \\leqslant accuracy < 0.91$ --- 1 балл;\n",
    "* $0.91 \\leqslant accuracy < 0.915$ --- 2 балла;\n",
    "* $0.915 \\leqslant accuracy$ --- 3 балла.\n",
    "\n",
    "### Оформление отчета (2 балла)\n",
    "В конце работы подробно опишите все проведенные эксперименты. \n",
    "* Укажите, какие из экспериментов принесли улучшение, а какие --- нет. \n",
    "* Проанализируйте графики сходимости моделей в проведенных экспериментах. Являются ли колебания качества обученных моделей существенными в зависимости от эпохи обучения, или же сходимость стабильная? \n",
    "* Укажите, какая модель получилась оптимальной.\n",
    "\n",
    "Желаем удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqKYrcyxBoxx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
